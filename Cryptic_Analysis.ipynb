{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e30ed4-8a0e-4a6f-96eb-dfe116dfb2de",
   "metadata": {},
   "source": [
    "#### Directories etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf9c6377-5080-4c22-94db-feabe17cc47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\Anaconda3\\envs\\Bioinformatics\\lib\\site-packages\\ete3-3.1.2-py3.7.egg\\ete3\\evol\\parser\\codemlparser.py:221: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\nicho\\Anaconda3\\envs\\Bioinformatics\\lib\\site-packages\\ete3-3.1.2-py3.7.egg\\ete3\\evol\\parser\\codemlparser.py:221: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.fft import fft, fftfreq\n",
    "from Bio import Entrez, SeqIO, AlignIO, pairwise2, Align, Seq, motifs\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation\n",
    "from scipy.stats import binom\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from Comparative_Analysis import Utilities as util\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from Comparative_Analysis import Blast_Functions as blastfn\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from Comparative_Analysis import ORF_Functions as orffn\n",
    "from random import sample\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import ete3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1988a242-f558-4643-b3a9-27272fa7bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = 'D:/Project_Data/Project_9'\n",
    "seq_dir = 'F:/Datasets/Actinobacteria_Ref_Rep_Lev_Complete'\n",
    "tb_species = 'NC_000962.3' \n",
    "tb_genome_filename = 'GCF_000195955.2_ASM19595v2_genomic.gbff'\n",
    "mycobrowser_dir = 'F:/Datasets/Data_From_Publications'\n",
    "num_cores = 16\n",
    "core_numbers = list(range(1, num_cores+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3235e1e1-a7fa-474b-8a88-72232e020b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in SeqIO.parse(seq_dir + '/' + tb_genome_filename, \"genbank\"):\n",
    "    full_sequence = str(record.seq)\n",
    "for record in SeqIO.parse(project_dir + '/annot.gbk', \"genbank\"):\n",
    "    reannotated_sequence = str(record.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba7e94a-f3f3-4b38-b620-c4a62e55efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mycobrowser_features =[]\n",
    "mycobrowser_df = pd.read_csv(mycobrowser_dir+'/Mycobrowser_Release_4.csv')\n",
    "for i, r in mycobrowser_df.iterrows():\n",
    "    if r['Strand'] == '+':\n",
    "        strand = 1\n",
    "    else:\n",
    "        strand = -1\n",
    "    mycobrowser_features.append([r['Locus'],r['Start']-1, r['Stop'], strand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bed02d4-5edf-46c9-bc55-cc30edc8028c",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_df = pd.read_csv(project_dir +'/Barcode.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b88b412-d21e-4178-aef9-84e81e3c0f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee7e9e-6a66-485f-badf-22c81560e77f",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30ebdfd2-85cc-4bc6-86c5-9e3c6c9cf3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_sequences(position_list, variant_dict, position_dict):\n",
    "    base_sequence = []\n",
    "    for i in position_list:\n",
    "        base_sequence.append(full_sequence[i-1])     # Cryptic are 1 based \n",
    "    output_sequence_dict ={}\n",
    "    for k, v in variant_dict.items():\n",
    "        output_sequence_dict[k] = copy.deepcopy(base_sequence)\n",
    "    for i, pos in enumerate(position_list):\n",
    "        if pos in position_dict:\n",
    "            variant_info = position_dict[pos][1:]  # Miss out initlal \"ref\" record\n",
    "            for (name, alt) in variant_info:\n",
    "                output_sequence_dict[name][i] = alt.upper()\n",
    "    output_sequences = []\n",
    "    for k, v in output_sequence_dict.items():\n",
    "        output_sequences.append(['seq_'+str(k), ''.join(v)])\n",
    "    return output_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fefd2694-16c3-4f59-b516-08d2b64a3082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_sequences_to_score(position_list, variant_dict, position_dict, distinct_sequence_names):\n",
    "    base_sequence = []\n",
    "    for i in position_list:\n",
    "        base_sequence.append({full_sequence[i-1]})     # Cryptic are 1 based \n",
    "    output_sequence_dict ={}\n",
    "    for k, v in variant_dict.items():\n",
    "        if k in distinct_sequence_names:\n",
    "            output_sequence_dict[k] = copy.deepcopy(base_sequence)\n",
    "    for i, pos in enumerate(position_list):\n",
    "        if pos in position_dict:\n",
    "            variant_info = position_dict[pos][1:]  # Miss out initlal \"ref\" record\n",
    "            for (name, alt) in variant_info:\n",
    "                if name in output_sequence_dict:\n",
    "                    output_sequence_dict[name][i] = {alt.upper()}\n",
    "    temp_dict = {}\n",
    "    for k, v in output_sequence_dict.items():\n",
    "        temp_dict['seq_'+str(k)] = v\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d9acb43-19f3-45e0-8214-f9481910cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitch_1(list_1, list_2):\n",
    "    res =[]\n",
    "    for i, j in zip(list_1, list_2):\n",
    "        a = i.intersection(j)\n",
    "        if len(a) == 0:\n",
    "            a = i.union(j)\n",
    "        res.append(a)\n",
    "    return res\n",
    "\n",
    "def fitch_2(parent_list, child_list):\n",
    "    res = []\n",
    "    mutations = []\n",
    "    for i, j in zip(parent_list, child_list):\n",
    "        mutation = 0\n",
    "        a = i.intersection(j)\n",
    "        if len(a) == 0:\n",
    "            a = set(list(j)[0])\n",
    "            mutation = 1\n",
    "        res.append(a)\n",
    "        if mutation == 1:\n",
    "            mutations.append(1)\n",
    "        else:\n",
    "            mutations.append(0)\n",
    "    return (res, mutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9db00dd-1420-47db-abea-32369fb5dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_formula(position_3_counts, tot_bin_counts):\n",
    "    return 1- binom.cdf(position_3_counts-1, tot_bin_counts,1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9eaf090-a2a6-4b2d-b054-602281be9016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mutation_bin_probability(mutation_counts):\n",
    "    bin_counts = [0,0,0]\n",
    "    for i, c in enumerate(mutation_counts):\n",
    "        bin_counts[i % 3] += c\n",
    "    if sum(bin_counts) == 0:\n",
    "        return (2)\n",
    "    else:\n",
    "        return (bin_counts, bin_formula(bin_counts[2], sum(bin_counts)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "461e5a4a-6849-4e60-b1d0-8b6c81e20610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculation_mutation_significance(start, stop):  \n",
    "    positions = list(range(start,stop))\n",
    "    sequence_to_score_dict = produce_sequences_to_score(positions, variant_dict, position_dict, distinct_sequence_names)\n",
    "\n",
    "    for node in master_tree.traverse(\"postorder\"):\n",
    "        if node.is_leaf():\n",
    "            node.add_features(seq = sequence_to_score_dict[node.name])\n",
    "        else:\n",
    "            children = node.children\n",
    "            node.add_features(seq = fitch_1(children[0].seq, children[1].seq))\n",
    "    for k, v in sequence_to_score_dict.items():              \n",
    "        seq_length = len(v)\n",
    "        break\n",
    "\n",
    "    mutation_counts = [0 for i in range(seq_length)]\n",
    "    for node in master_tree.traverse(\"preorder\"):\n",
    "        if node.is_leaf():\n",
    "            continue\n",
    "        if node.is_root():\n",
    "            node.seq = [{list(x)[0]} for x in node.seq]\n",
    "        children = node.children\n",
    "        mutations = []\n",
    "        for child in children:\n",
    "            (temp_1, temp_2) = fitch_2(node.seq ,child.seq)\n",
    "            child.seq = temp_1\n",
    "            mutations.append(temp_2)\n",
    "        temp = []\n",
    "        for h, i, j in zip(mutation_counts, mutations[0], mutations[1]):\n",
    "            temp.append(h+max(i, j))\n",
    "        mutation_counts = temp        \n",
    "    return mutation_bin_probability(mutation_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95cb8d3e-ef11-4e1a-bf89-cacff5ff6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mutation_counts(filename):    \n",
    "    a = filename.split('_')\n",
    "    start = a[-3]\n",
    "    stop = a[-2]\n",
    "    with open(filename, 'rb') as f:\n",
    "        sequence_to_score_dict = pickle.load(f)\n",
    "    for node in master_tree.traverse(\"postorder\"):\n",
    "        if node.is_leaf():\n",
    "            node.add_features(seq = sequence_to_score_dict[node.name])\n",
    "        else:\n",
    "            children = node.children\n",
    "            node.add_features(seq = fitch_1(children[0].seq, children[1].seq))\n",
    "    for k, v in sequence_to_score_dict.items():              \n",
    "        seq_length = len(v)\n",
    "        break\n",
    "\n",
    "    mutation_counts = [0 for i in range(seq_length)]\n",
    "    for node in master_tree.traverse(\"preorder\"):\n",
    "        if node.is_leaf():\n",
    "            continue\n",
    "        if node.is_root():\n",
    "            node.seq = [{list(x)[0]} for x in node.seq]\n",
    "        children = node.children\n",
    "        mutations = []\n",
    "        for child in children:\n",
    "            (temp_1, temp_2) = fitch_2(node.seq ,child.seq)\n",
    "            child.seq = temp_1\n",
    "            mutations.append(temp_2)\n",
    "        temp = []\n",
    "        for h, i, j in zip(mutation_counts, mutations[0], mutations[1]):\n",
    "            temp.append(h+max(i, j))\n",
    "        mutation_counts = temp        \n",
    "    return (start, stop, mutation_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc25129-08e0-4ce1-9b17-a64f477c50d8",
   "metadata": {},
   "source": [
    "#### Create variant dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5fcbb04-2c85-4719-b420-2651cf1e2cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    variant_df = pd.read_csv(cryptic_input_path + \"VARIANTS.csv\") \n",
    "    with open(project_dir + '/variant_df.pkl', 'wb') as f:\n",
    "        pickle.dump(variant_df[['UNIQUEID', 'VARIANT', 'MUTATION_TYPE', 'IS_NULL', 'IS_HET', 'IS_FILTER_PASS', 'IS_SNP', 'REF', 'ALT', 'GENOME_INDEX']], f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77fe61cc-adfe-4aa1-97d1-ba7ef7ac00ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    position_dict = {}\n",
    "    variant_dict = {}\n",
    "    id_dict = {}\n",
    "    with open(project_dir + '/variant_df.pkl', 'rb') as f:\n",
    "        variant_df = pickle.load(f) \n",
    "        unique_ids = variant_df.UNIQUEID.unique()\n",
    "        for i, unique_id in enumerate(unique_ids):\n",
    "            id_dict[unique_id] = i\n",
    "\n",
    "        for i, r in variant_df.iterrows():\n",
    "            if r['IS_NULL'] == False and r['IS_FILTER_PASS'] == True and r['IS_HET'] == False and r['IS_SNP'] == True :\n",
    "                \n",
    "                if id_dict[r['UNIQUEID']] in variant_dict:\n",
    "                    variant_dict[id_dict[r['UNIQUEID']]].append((r['GENOME_INDEX'], r['ALT']))\n",
    "                else:\n",
    "                    variant_dict[id_dict[r['UNIQUEID']]] = [(r['GENOME_INDEX'], r['ALT'])]\n",
    "\n",
    "                if r['GENOME_INDEX'] in position_dict:\n",
    "                    position_dict[r['GENOME_INDEX']].append((id_dict[r['UNIQUEID']], r['ALT']))\n",
    "                else:\n",
    "                    position_dict[r['GENOME_INDEX']] = [r['REF'], (id_dict[r['UNIQUEID']], r['ALT'])]    # If first entry also include reference value for info\n",
    "\n",
    "    with open(project_dir + '/id_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(id_dict, f)\n",
    "    with open(project_dir + '/variant_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(variant_dict, f) \n",
    "    with open(project_dir + '/position_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(position_dict, f) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef9da2b4-26ab-4150-90e6-99fc3b388cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == False:\n",
    "    with open(project_dir + '/id_dict.pkl', 'rb') as f:\n",
    "        id_dict = pickle.load(f)  \n",
    "    with open(project_dir + '/variant_dict.pkl', 'rb') as f:\n",
    "        variant_dict = pickle.load(f)  \n",
    "    with open(project_dir + '/position_dict.pkl', 'rb') as f:\n",
    "        position_dict = pickle.load(f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f26106-6d45-411e-b4bd-e5f796e37793",
   "metadata": {},
   "source": [
    "#### Output full sequences and distinct sequences based on barcode positions for construction of tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9a14de1-b8a7-495a-97f4-42b60a5e379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "barcode_snps = []\n",
    "for i, r in barcode_df.iterrows():\n",
    "    start = r['start'] + 1            # Cryptic file is 1 indexed\n",
    "    lineage_name = r['lineage']\n",
    "    if start in position_dict and 'lineage' in lineage_name:   # Just TB lineages\n",
    "        barcode_snps.append(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c0306d9-c288-43c4-825f-4d5044db6a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "985"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(barcode_snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02308c27-c3ad-4ae6-92ec-951cec9a27c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3596"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "barcode_snps = []\n",
    "for k, v in position_dict.items():\n",
    "    if len(v) > 2000:\n",
    "        barcode_snps.append(k)\n",
    "len(barcode_snps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "935875cd-ea0e-4d02-934e-38727f7c0881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77580/77580 [00:02<00:00, 34238.40it/s]\n"
     ]
    }
   ],
   "source": [
    "sequences = produce_sequences(barcode_snps, variant_dict, position_dict)\n",
    "util.produce_fasta_file(sequences, project_dir + '/' + 'tb_variants.faa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9468f0da-f0af-48d1-b9ee-2fc476fc3169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58252/58252 [00:01<00:00, 33946.95it/s]\n"
     ]
    }
   ],
   "source": [
    "distinct_sequences = []\n",
    "distinct_sequence_names = []\n",
    "temp_dict = {}\n",
    "for (ref, seq) in sequences:\n",
    "    temp_dict[seq] = ref\n",
    "for k, v in temp_dict.items():    \n",
    "    distinct_sequences.append([v, k])\n",
    "    distinct_sequence_names.append(int(v.split('_')[1]))\n",
    "util.produce_fasta_file(distinct_sequences, project_dir + '/' + 'distinct_tb_variants.faa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a87d12-a23d-4a2c-9c78-d1bea113586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_tree = ete3.Tree(project_dir + '/' + 'distinct_tb_variants.nwk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38020a2d-eb43-4999-a94a-d887fcbba4a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Produce and save mutations per position (first produce files containing blocks of 10,000 nt values for all sequences in tree - will take about 36 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1bc63-f5ae-4208-9a5f-329ac6bb3a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    small_variant_dict = {}\n",
    "    for k, v in variant_dict.items():\n",
    "        if k in distinct_sequence_names:\n",
    "            small_variant_dict[k] = vtb_sequence_length = len(full_sequence)\n",
    "    chunk_size = 10000\n",
    "    num_chunks = math.ceil(tb_sequence_length/chunk_size)\n",
    "    for chunk in tqdm(range(num_chunks)):\n",
    "        start_pos = chunk * chunk_size\n",
    "        end_pos = min(tb_sequence_length, start_pos + chunk_size)\n",
    "        variant_sequence_dict = {}\n",
    "        for k, v in small_variant_dict.items():\n",
    "            temp = list(full_sequence)\n",
    "            for (pos, snp) in v:\n",
    "                temp[pos-1] = snp.upper()\n",
    "            variant_sequence_dict['seq_'+str(k)] = [{x} for x in temp[start_pos:end_pos]]\n",
    "        with open(project_dir + '/Cryptic_Sequence_Dictionaries/variant_dict_'+str(start_pos)+'_'+str(end_pos)+'_' + '.pkl', 'wb') as f:\n",
    "                pickle.dump(variant_sequence_dict, f) \n",
    "                \n",
    "    filename_list = util.list_files(project_dir+'/Cryptic_Sequence_Dictionaries')\n",
    "    res = []\n",
    "    start_time = time.time()\n",
    "    for filename in tqdm(filename_list):\n",
    "        temp = generate_mutation_counts(project_dir+'/Cryptic_Sequence_Dictionaries/'+filename)\n",
    "        res.append(temp)\n",
    "    with open(project_dir + '/mutation_counts.pkl', 'wb') as f:\n",
    "                pickle.dump(res, f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1740e557-9786-42c4-8b3e-1ee1f9e5df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == False:\n",
    "    with open(project_dir + '/mutation_counts.pkl', 'rb') as f:\n",
    "        mutation_counts = pickle.load(f)  \n",
    "mutation_counts.sort(key = lambda x: int(x[0]))\n",
    "mutation_count_list = []\n",
    "for (start, stop, counts) in mutation_counts:\n",
    "    for count in counts:\n",
    "        mutation_count_list.append(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240d70ef-14d8-4bf2-a360-0e611506a410",
   "metadata": {},
   "source": [
    "#### Calculate probabilites for annotated (and reannotated PGAP) CDS regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b3bf0-ecda-4573-8e66-7fcde8acd18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cds_boundaries = []\n",
    "for genome_record in SeqIO.parse(seq_dir + '/' + tb_genome_filename, \"genbank\"):\n",
    "    for feature in genome_record.features:\n",
    "        if feature.type == 'CDS':\n",
    "            a = feature.qualifiers  \n",
    "            if a.get(\"pseudo\") == None:\n",
    "                pseudo = False\n",
    "            else:\n",
    "                pseudo = True\n",
    "            cds_boundaries.append((a.get(\"locus_tag\")[0], pseudo, a.get(\"product\")[0], int(feature.location.start), int(feature.location.end), int(feature.location.strand)))   \n",
    "reannotated_cds_boundaries = []\n",
    "for genome_record in SeqIO.parse(project_dir + '/annot.gbk', \"genbank\"):\n",
    "    for feature in genome_record.features:\n",
    "        if feature.type == 'CDS':\n",
    "            a = feature.qualifiers  \n",
    "            if a.get(\"pseudo\") == None:\n",
    "                pseudo = False\n",
    "            else:\n",
    "                pseudo = True\n",
    "            reannotated_cds_boundaries.append((a.get(\"locus_tag\")[0], pseudo, a.get(\"product\")[0], int(feature.location.start), int(feature.location.end), int(feature.location.strand)))   \n",
    "cds_boundaries.sort(key = lambda x: x[3])\n",
    "reannotated_cds_boundaries.sort(key = lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77c9d1e-8d02-4ed8-9d7f-2f599b5f1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp =[]\n",
    "for (locus, pseudo, product, start, stop, strand) in cds_boundaries:\n",
    "    if pseudo == False:\n",
    "        if strand == 1:\n",
    "            temp.append(mutation_bin_probability(mutation_count_list[start:stop]))\n",
    "        else:\n",
    "            temp.append(mutation_bin_probability(reversed(mutation_count_list[start:stop])))\n",
    "scores = []\n",
    "for x in temp:\n",
    "    if x == 2:\n",
    "        scores.append(x)\n",
    "    else:\n",
    "        scores.append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b9d7e-f827-4b4d-921e-f63b44f50ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(scores, bins =100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6321f-5a4e-481b-80dd-b73ec38fbd18",
   "metadata": {},
   "source": [
    "#### Identify potential ORFS (min length 200) in inter-CDS regions of standard annotation and output to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3b790-e301-4599-a155-74a0e7ae05ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORFFinder = orffn.ORF_Finder()\n",
    "trans = util.Translator()\n",
    "min_intergenic_length = 100\n",
    "details =  []\n",
    "results =[]\n",
    "for i, (locus, pseudo, product, start, stop, strand) in enumerate(cds_boundaries):\n",
    "    if i < len(cds_boundaries) - 1:\n",
    "        if cds_boundaries[i+1][3] > stop + min_intergenic_length:\n",
    "            a =ORFFinder.max_orf(stop-40, cds_boundaries[i+1][3]+40, 1e-20, output_all_orfs = False, min_orf_length = 200)\n",
    "            if not(a==(0,0,0)):\n",
    "                ov = 0\n",
    "                info = ('','','','','','','')\n",
    "                for i, (loc, pse, pro, sta, sto, stra) in enumerate(reannotated_cds_boundaries):\n",
    "                    if a[1] > sta and a[0] < sto:\n",
    "                        ov = (min(a[1], sto) - max(a[0], sta)) / (sto - sta)\n",
    "                        if ov > 0.3:\n",
    "                            info =  (loc, pse, pro, sta, sto, stra, ov)\n",
    "                        \n",
    "                ov = 0\n",
    "                myco_info = ('','','','','')\n",
    "                for i, (loc, sta, sto, stra) in enumerate(mycobrowser_features):\n",
    "                    if a[1] > sta and a[0] < sto:\n",
    "                        ov = (min(a[1], sto) - max(a[0], sta)) / (sto - sta)\n",
    "                        if ov > 0.3:\n",
    "                            myco_info =  (loc, sta, sto, stra, ov)\n",
    "                        \n",
    "                sequ = trans.translate_sequence(full_sequence[a[0]:a[1]], a[2], 0)\n",
    "                details.append([a, sequ])\n",
    "                results.append([a[0],a[1],a[2],a[3],info[0],info[1],info[2],info[3],info[4],info[5],info[6],myco_info[0],myco_info[1],myco_info[2],myco_info[3],myco_info[4]])\n",
    "results_df = pd.DataFrame(results, columns = ['start_pos','end_pos','strand','score','PGAP_ref','PGAP_pseudogene','PGAP_product', 'PGAP_start', 'PGAP_end','PGAP_strand', 'PGAP_overlap', 'Mycob_ref','Mycob_start', 'Mycob_end','Mycob_strand', 'Mycob_overlap'])\n",
    "results_df.to_csv(project_dir + '/cds_candidates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1542c9b-7d8d-4bcc-8104-edf07608f227",
   "metadata": {},
   "source": [
    "#### Calculate probabilities for regions in Smith et al 2021 and plot histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe8a8a4-503a-43e1-86a7-48a9cdb0155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile('F:/Datasets/Data_From_Publications/Smith_2021.xlsx')\n",
    "df1 = pd.read_excel(xls, 'Table S3', header=3)\n",
    "co_ords = []\n",
    "for i, r in df1.iterrows():\n",
    "    if r['Classification'] == 'Novel':\n",
    "        if r['Strand'] == '+':\n",
    "            co_ords.append([int(r['Start Coordinate']-1), int(r['Stop Coordinate']), 1])\n",
    "        else:\n",
    "            co_ords.append([int(r['Stop Coordinate'] - 1), int(r['Start Coordinate']),-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58096b14-401d-45d2-bf2f-4a45c6b02730",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "probs = []\n",
    "for x in co_ords:\n",
    "    if x[2] == 1:\n",
    "        a = (mutation_bin_probability(mutation_count_list[x[0]:x[1]]))\n",
    "    else:\n",
    "        a = (mutation_bin_probability(reversed(mutation_count_list[x[0]:x[1]])))\n",
    "    if a == 2:\n",
    "        probs.append(2)\n",
    "    else:\n",
    "        probs.append(a[1])\n",
    "    print(x, abs(x[1]-x[0]), a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c42903-2248-4181-aa6a-99cb62c354ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(probs, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ce0ae-2b01-482b-83cb-93c720537916",
   "metadata": {},
   "source": [
    "#### Find all (maximal nested) ORFs and filter out ORFS on opposite strand which would have same non-synonymous positions with larger ORF on other strand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea12a609-41c4-402f-a220-c09b6cc10fb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ORFFinder = orffn.ORF_Finder()\n",
    "a = ORFFinder.max_orf(0, 4411532, 5, output_orfs = 'Nested', min_orf_length = 50)\n",
    "a.sort(key = lambda x: x[3], reverse = True)\n",
    "orf_list = [a[0]]\n",
    "for x in tqdm(a[1:]):\n",
    "    matched = 0\n",
    "    for v in orf_list:\n",
    "        if v[0]<=x[0] and v[1]>=x[1]:\n",
    "            if x[2] == v[2]:\n",
    "                if (v[0] - x[0])%3 == 0:\n",
    "                    matched = 1\n",
    "                    break\n",
    "            else:\n",
    "                if (v[0] - x[0])%3 == 1:\n",
    "                    matched = 1\n",
    "                    break\n",
    "    if matched == 0:\n",
    "        orf_list.append(x)\n",
    "orf_list.sort(key = lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22d62d-f297-4076-b929-ee932e041fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = []\n",
    "for x in orf_list:\n",
    "    prob.append(x[4])\n",
    "sns.histplot(prob, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2769ef-9358-406c-a22a-e88be6b93338",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_features = []\n",
    "for genome_record in SeqIO.parse(project_dir + '/annot.gbk', \"genbank\"):\n",
    "    for feature in genome_record.features:\n",
    "        if feature.type != 'source':\n",
    "            annotated_features.append((int(feature.location.start), int(feature.location.end), int(feature.location.strand)))   \n",
    "for genome_record in SeqIO.parse(seq_dir + '/' + tb_genome_filename, \"genbank\"):\n",
    "    for feature in genome_record.features:\n",
    "        if feature.type != 'source':\n",
    "            annotated_features.append((int(feature.location.start), int(feature.location.end), int(feature.location.strand)))   \n",
    "annotated_features.sort(key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd77402-5afe-4b13-9f1e-1a14e569d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_overlapping_orfs = []\n",
    "for i, orf in enumerate(orf_list):\n",
    "    max_ov = 0\n",
    "    for (sta, sto, stra) in annotated_features:\n",
    "        if orf[0] < sto and orf[1] > sta:\n",
    "            ov = (min(orf[1], sto) - max(orf[0], sta)) / (orf[1] - orf[0])\n",
    "            max_ov = max(ov, max_ov)\n",
    "    if max_ov < 0.1:\n",
    "        non_overlapping_orfs.append(orf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2683970-7a45-419b-875a-0ca44f8b7a66",
   "metadata": {},
   "source": [
    "#### Produce FASTA file with CDS candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564e695-944e-448f-b6e1-4936f0ae0f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_overlapping_orfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214885e2-505b-42a5-a423-dd784b69cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = util.Translator()\n",
    "temp = []\n",
    "for x in non_overlapping_orfs:\n",
    "    if x[4] < 1e-5 or x[4]==2:\n",
    "        if x[2] == 1:\n",
    "            prot = trans.translate_sequence(full_sequence[x[0]:x[1]], 1, 0)\n",
    "        else:\n",
    "            prot = trans.translate_sequence(util.reverse_complement(full_sequence[x[0]:x[1]]), 1, 0)\n",
    "        name = 'Start_'+str(x[0])+'_Stop_'+str(x[1])+'_Strand_'+str(x[2])\n",
    "        temp.append([name, prot[:-1]])\n",
    "util.produce_fasta_file(temp, project_dir + '/' + 'tb_orf_candidates.faa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dbfb10-9277-4b21-9446-8fee25d4271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "blastfn.run_tblastn('F:/Datasets/BLAST/actinobacteria_ref_genomes', 'tb_orf_candidates.faa', 'blastdb_sourceseq_actinobacteria', e_value = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b5056-e487-4742-8f14-b64008c781ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = []\n",
    "for x in orf_list:\n",
    "    prob.append(x[4])\n",
    "sns.histplot(prob, bins =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93696eef-27de-4070-b4d8-3a47e7319a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [x for x in orf_list if x[4] <1e-3]\n",
    "len(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0289a929-0d29-4246-a700-44840af34050",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Legacy code (might be useful - this is when tree was defined based on \"optimal splits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2cb92ea-0b23-4121-be3c-8f355b0be4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991419 32277\n"
     ]
    }
   ],
   "source": [
    "reduced_position_dict =  {}\n",
    "for k, v in position_dict.items():\n",
    "    if len(v) >100:\n",
    "        reduced_position_dict[k] = v\n",
    "print(len(position_dict), len(reduced_position_dict))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9ffb9d2-7d32-4666-8b4f-f74a2f4a441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tree(species_to_split, position):\n",
    "    temp =  set([x[0] for x in reduced_position_dict[position][1:]]).intersection(species_to_split)\n",
    "    temp_2 = species_to_split - temp\n",
    "    if len(temp_2) > 1 and len(temp) > 1:\n",
    "        return ([(position, temp, len(temp)), (position, temp_2, len(temp_2))])\n",
    "    else:\n",
    "        return([(-1, species_to_split, len(species_to_split))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91cb7982-32e0-4eb1-afab-04b940de7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_split_position(species_to_split_list):\n",
    "    best_split_num = 1e20\n",
    "    best_position = 0\n",
    "    for k, v in reduced_position_dict.items():\n",
    "        mutation_count = 0\n",
    "        for species_to_split in species_to_split_list:\n",
    "            num_species_to_split = len(species_to_split)\n",
    "            optimal_split = int(num_species_to_split/2)\n",
    "            mutation_count += abs(len(set([x[0] for x in v[1:]]).intersection(species_to_split)) - optimal_split)\n",
    "       \n",
    "        if abs(mutation_count) < abs(best_split_num):\n",
    "            best_position = k\n",
    "            best_split_num = abs(mutation_count)\n",
    "          \n",
    "    return(best_position, best_split_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d79dd452-3ecc-4b58-9fa0-1f8950d400fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(836658, 38166), (836658, 39686)]\n",
      "20334\n",
      "11.734375\n",
      "[(1390763, 18111), (1390763, 20055), (1390763, 19352), (1390763, 20334)]\n",
      "13018\n",
      "18.53125\n",
      "[(1993808, 6884), (1993808, 11227), (1993808, 10585), (1993808, 9470), (1993808, 6334), (1993808, 13018), (1993808, 11005), (1993808, 9329)]\n",
      "9513\n",
      "33.390625\n",
      "[(3943019, 4813), (3943019, 2071), (3943019, 8015), (3943019, 3212), (3943019, 7829), (3943019, 2756), (3943019, 5467), (3943019, 4003), (3943019, 1714), (3943019, 4620), (3943019, 3505), (3943019, 9513), (3943019, 2411), (3943019, 8594), (3943019, 2198), (3943019, 7131)]\n",
      "6714\n",
      "60.234375\n",
      "[(2155168, 567), (2155168, 4246), (2155168, 249), (2155168, 1822), (2155168, 3241), (2155168, 4774), (2155168, 1419), (2155168, 1793), (2155168, 2020), (2155168, 5809), (2155168, 605), (2155168, 2151), (2155168, 883), (2155168, 4584), (2155168, 478), (2155168, 3525), (2155168, 144), (2155168, 1570), (2155168, 710), (2155168, 3910), (2155168, 1322), (2155168, 2183), (2155168, 4501), (2155168, 5012), (2155168, 391), (2155168, 2020), (2155168, 1880), (2155168, 6714), (2155168, 269), (2155168, 1929), (2155168, 1150), (2155168, 5981)]\n",
      "4864\n",
      "112.53125\n",
      "[(1779370, 437), (1779370, 130), (1779370, 3192), (1779370, 1054), (1779370, 217), (1779370, 32), (1779370, 1463), (1779370, 359), (1779370, 132), (1779370, 3109), (1779370, 910), (1779370, 3864), (1779370, 81), (1779370, 1338), (1779370, 615), (1779370, 1178), (1779370, 1037), (1779370, 983), (1779370, 2387), (1779370, 3422), (1779370, 249), (1779370, 356), (1779370, 1055), (1779370, 1096), (1779370, 234), (1779370, 649), (1779370, 992), (1779370, 3592), (1779370, 116), (1779370, 362), (1779370, 412), (1779370, 3113), (1779370, 114), (1779370, 30), (1779370, 993), (1779370, 577), (1779370, 589), (1779370, 121), (1779370, 3017), (1779370, 893), (1779370, 33), (1779370, 1289), (1779370, 245), (1779370, 1938), (1779370, 232), (1779370, 4269), (1779370, 1076), (1779370, 3936), (1779370, 169), (1779370, 222), (1779370, 1078), (1779370, 942), (1779370, 612), (1779370, 1268), (1779370, 2823), (1779370, 3891), (1779370, 55), (1779370, 214), (1779370, 347), (1779370, 1582), (1779370, 256), (1779370, 894), (1779370, 1117), (1779370, 4864)]\n",
      "3832\n",
      "213.046875\n",
      "[(2123169, 390), (2123169, 47), (2123169, 113), (2123169, 17), (2123169, 2601), (2123169, 591), (2123169, 888), (2123169, 166), (2123169, 186), (2123169, 31), (2123169, 25), (2123169, 7), (2123169, 1269), (2123169, 194), (2123169, 294), (2123169, 65), (2123169, 130), (2123169, 2), (2123169, 3060), (2123169, 49), (2123169, 870), (2123169, 40), (2123169, 3832), (2123169, 32), (2123169, 69), (2123169, 12), (2123169, 1272), (2123169, 66), (2123169, 581), (2123169, 34), (2123169, 1143), (2123169, 35), (2123169, 948), (2123169, 89), (2123169, 947), (2123169, 36), (2123169, 2028), (2123169, 359), (2123169, 3297), (2123169, 125), (2123169, 207), (2123169, 42), (2123169, 306), (2123169, 50), (2123169, 866), (2123169, 189), (2123169, 1014), (2123169, 82), (2123169, 223), (2123169, 11), (2123169, 624), (2123169, 25), (2123169, 896), (2123169, 96), (2123169, 3443), (2123169, 149), (2123169, 110), (2123169, 6), (2123169, 335), (2123169, 27), (2123169, 304), (2123169, 108), (2123169, 1998), (2123169, 1115), (2123169, 69), (2123169, 45), (2123169, 25), (2123169, 5), (2123169, 718), (2123169, 275), (2123169, 484), (2123169, 93), (2123169, 166), (2123169, 423), (2123169, 54), (2123169, 67), (2123169, 1727), (2123169, 1290), (2123169, 591), (2123169, 302), (2123169, 29), (2123169, 4), (2123169, 1054), (2123169, 235), (2123169, 226), (2123169, 19), (2123169, 1848), (2123169, 90), (2123169, 78), (2123169, 154), (2123169, 1409), (2123169, 2860), (2123169, 728), (2123169, 348), (2123169, 1935), (2123169, 2001), (2123169, 100), (2123169, 69), (2123169, 187), (2123169, 35), (2123169, 828), (2123169, 250), (2123169, 874), (2123169, 68), (2123169, 208), (2123169, 404), (2123169, 435), (2123169, 833), (2123169, 1342), (2123169, 1481), (2123169, 2343), (2123169, 1548), (2123169, 51), (2123169, 4), (2123169, 173), (2123169, 41), (2123169, 310), (2123169, 37), (2123169, 1437), (2123169, 145), (2123169, 117), (2123169, 139), (2123169, 312), (2123169, 582), (2123169, 615), (2123169, 502), (2123169, 2504), (2123169, 2360)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33060/744716093.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtemp_split_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplit_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mposn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimal_split_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplit_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mposn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0msplit_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mposn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33060/418184240.py\u001b[0m in \u001b[0;36moptimal_split_position\u001b[1;34m(species_to_split_list)\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mnum_species_to_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspecies_to_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0moptimal_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_species_to_split\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mmutation_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspecies_to_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moptimal_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmutation_count\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_split_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_species = [[-1,set(x for x in range(len(id_dict))),99,True]]\n",
    "pos = optimal_split_position([x[1] for x in all_species])[0]\n",
    "split_results = [split_tree(all_species[0][1], pos)]\n",
    "print( [(x[0], x[2]) for x in split_results[0]] )\n",
    "for i in range(1,50):\n",
    "    start = time.process_time()\n",
    "    split_results.append([])\n",
    "    temp_split_results = [x[1] for x in split_results[i-1]]\n",
    "      \n",
    "    posn = optimal_split_position([x[1] for x in split_results[i-1]])   \n",
    "    pos= posn[0]\n",
    "    split_score = posn[1]\n",
    "    successful_splits = 0\n",
    "    for x in split_results[i-1]:\n",
    "        temp = split_tree(x[1], pos) \n",
    "        if len(temp) == 1:\n",
    "            split_results[i].append(temp[0])\n",
    "        else:\n",
    "            successful_splits +=1\n",
    "            split_results[i].append(temp[0])\n",
    "            split_results[i].append(temp[1])\n",
    "    print(max([x[2] for x in split_results[i]]))\n",
    "    print (time.process_time() - start)\n",
    "    print([(x[0], x[2]) for x in split_results[i]])\n",
    "    if successful_splits == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf357d-cf35-41aa-9408-3c7c067ba2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x[0],x[2]) for x in split_results[2] if x[2] > 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b74d58-14a0-443a-96ab-8aa78a0fe0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for res in split_results:\n",
    "    for info in res:\n",
    "        temp.append(info[0])\n",
    "snps = set(temp)\n",
    "snps.remove(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf9a4de-bd5b-40b8-a247-9a907ed4c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mutation_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25046046-2644-4c23-aaa4-8176fb887d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mutation_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431024b-eb44-45a3-b7b5-4e5b2a38a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(mutation_count_list, bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f331c-d6de-40d1-9cb5-8362c832e141",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [x[1] for x in split_results[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc3f00-fb7d-4304-a5e1-0d94b4559401",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f684a584-361d-42d3-9cf4-45cbed3e5b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77580/77580 [00:47<00:00, 1627.44it/s]\n"
     ]
    }
   ],
   "source": [
    "distinct_ids = []\n",
    "variants = []\n",
    "for k, v in tqdm(variant_dict.items()):\n",
    "    if set(v) in variants:\n",
    "        continue\n",
    "    else:\n",
    "        variants.append(set(v))\n",
    "        distinct_ids.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ca14ce1-f2b9-4a70-91b0-8e3f0d2f637d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77580"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(variant_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88834d4c-38af-4314-b4ea-9f99491bb946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71054"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distinct_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "618ef69d-d9a6-4cfd-aa4b-d9248c9aafaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9db1ff2b-c3e3-4011-b49b-fbdb29bd9436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1991419"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(position_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f2c251-666a-428b-94bd-ba7bb5f5ab53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
