{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50da8fa4-e4d1-4e99-9df9-f2593cc0c057",
   "metadata": {},
   "source": [
    "##### Import modules and set up file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9417e01-aa2a-4297-8948-c0c526434b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "694b2495-3118-4d6a-9934-5f46d80dfa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\Anaconda3\\envs\\Projects\\lib\\site-packages\\ete3-3.1.2-py3.7.egg\\ete3\\evol\\parser\\codemlparser.py:221: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\nicho\\Anaconda3\\envs\\Projects\\lib\\site-packages\\ete3-3.1.2-py3.7.egg\\ete3\\evol\\parser\\codemlparser.py:221: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"
     ]
    }
   ],
   "source": [
    "from Comparative_Analysis import Sequence_Analysis_Routines as sar\n",
    "from Comparative_Analysis import HMM as hmm\n",
    "from Comparative_Analysis import Utilities as util\n",
    "from Comparative_Analysis import Alignment_HMM as alignment_hmm\n",
    "from Comparative_Analysis import Alignment_Analysis as alignment_analysis\n",
    "from Comparative_Analysis import Alignment as align\n",
    "from Comparative_Analysis import Master_Alignment_HMM as master_alignment_hmm\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import optimize as opt\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import logomaker as lm\n",
    "import math\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import ete3;\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "773a1d9c-8b02-447d-8b4c-41e9d5d24f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = 'D:/Project_Data/Project_3'\n",
    "sonic_paranoid_run_name = 'Run_Without_Outgroup'\n",
    "outgroup_sonic_paranoid_run_name = 'Run_With_Outgroup'\n",
    "genome_datasets_dir = project_dir + '/Datasets/NCBI_Datasets_Close_Species/'\n",
    "output_dir = project_dir + '/Output/Close_Species'\n",
    "protein_fasta_output_loc = output_dir + '/Protein_Sequences'\n",
    "outgroup_protein_fasta_output_loc = output_dir + '/Protein_Sequences_With_Outgroup'\n",
    "sonic_paranoid_output_loc = output_dir + '/Sonic_Paranoid_Output'\n",
    "ortholog_file_dir = sonic_paranoid_output_loc + '/runs/' + sonic_paranoid_run_name + '/ortholog_groups'\n",
    "outgroup_ortholog_dir = sonic_paranoid_output_loc + '/runs/' + outgroup_sonic_paranoid_run_name + '/ortholog_groups'\n",
    "non_cds_output_dir = output_dir + '/Multiple_Alignment_Data/Non_CDS'\n",
    "upstream_non_cds_output_dir = output_dir + '/Multiple_Alignment_Data/Upstream_Non_CDS'\n",
    "cds_output_dir = output_dir + '/Multiple_Alignment_Data/CDS'\n",
    "extended_cds_output_dir = output_dir + '/Multiple_Alignment_Data/Extended_CDS'\n",
    "outgroup_cds_output_dir = output_dir + '/Multiple_Alignment_Data/CDS_With_Outgroup'\n",
    "outgroup_concatenated_cds_output_dir = output_dir + '/Multiple_Alignment_Data/CDS_With_Outgroup_Concatenated'\n",
    "hmm_parameters_output_dir = output_dir +'/HMM_Model_Parameters'\n",
    "conservation_analysis_output_dir = output_dir + '/Conservation_Analysis_Output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56139988-efa1-4bb1-9cd3-4cdd90020453",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 16\n",
    "core_numbers = list(range(1, num_cores+1))\n",
    "non_cds_offset = 50\n",
    "extended_cds_offset = 100\n",
    "tb_species = 'GCF_000195955.2'\n",
    "outgroup_species = 'GCF_000696675.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf829bb3-e529-4170-ad7b-9d885d57f851",
   "metadata": {},
   "source": [
    "##### Determine genomes in ortholog family, generate protein files and run Sonic Paranoid (both with and without outgroup - outgroup needed for tree building)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7a7627-bb45-4679-97ef-ed8f209447fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_ids_with_outgroup = util.list_dirs(genome_datasets_dir)\n",
    "genome_ids = util.list_dirs(genome_datasets_dir)\n",
    "genome_ids.remove(outgroup_species)\n",
    "non_target_genome_ids = util.list_dirs(genome_datasets_dir)\n",
    "non_target_genome_ids.remove(outgroup_species)\n",
    "non_target_genome_ids.remove(tb_species)\n",
    "num_ids = len(genome_ids)\n",
    "num_ids_with_outgroup = len(genome_ids_with_outgroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be93725-4667-49ca-b2bc-430c0c7e9eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    for folder in sar.tqdm(genome_ids):\n",
    "        sar.generate_protein_file(genome_datasets_dir + '/' + folder + '/genomic.gbff', protein_fasta_output_loc + '/' + folder + '.faa')\n",
    "    for folder in sar.tqdm(genome_ids_with_outgroup):\n",
    "        sar.generate_protein_file(genome_datasets_dir + '/' + folder + '/genomic.gbff', outgroup_protein_fasta_output_loc + '/' + folder + '.faa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f828d159-34d6-4f7e-878c-e31b598573a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    sar.run_sonic_paranoid(protein_fasta_output_loc, sonic_paranoid_output_loc, sonic_paranoid_run_name)\n",
    "    sar.run_sonic_paranoid(outgroup_protein_fasta_output_loc, sonic_paranoid_output_loc, outgroup_sonic_paranoid_run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39369771-8cd9-4438-89a5-d426702ac448",
   "metadata": {},
   "source": [
    "##### Generate objects containing orthologs and sequence information for each ortholog group / species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "051a50d7-e6dc-42a7-9675-e58d8ae157b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/Project_Data/Project_3/Output/Close_Species/Sonic_Paranoid_Output/runs/Run_Without_Outgroup/ortholog_groups/flat.ortholog_groups.tsv/flat.ortholog_groups.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20720/3123259094.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0morthologs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrtholog_Grouping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mortholog_file_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0moutgroup_orthologs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOrtholog_Grouping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutgroup_ortholog_file_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Sequence_Conservation\\Comparative_Analysis\\Sequence_Analysis_Routines.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file_loc)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mOrtholog_Grouping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0morthologs_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_loc\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'/flat.ortholog_groups.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munassigned_genes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_loc\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'/not_assigned_genes.ortholog_groups.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mfile_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morthologs_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Projects\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Projects\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Projects\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Projects\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Projects\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Projects\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/Project_Data/Project_3/Output/Close_Species/Sonic_Paranoid_Output/runs/Run_Without_Outgroup/ortholog_groups/flat.ortholog_groups.tsv/flat.ortholog_groups.tsv'"
     ]
    }
   ],
   "source": [
    "orthologs = sar.Ortholog_Grouping(ortholog_file_dir)\n",
    "outgroup_orthologs = sar.Ortholog_Grouping(outgroup_ortholog_file_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9977fb-f334-4f33-a820-535d87f4105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data = sar.Ortholog_Sequence_Dataset(orthologs, genome_datasets_dir, genome_ids, non_cds_offset, tb_species) \n",
    "outgroup_seq_data = sar.Ortholog_Sequence_Dataset(outgroup_orthologs, genome_datasets_dir, genome_ids_with_outgroup, non_cds_offset, tb_species) \n",
    "all_copy_seq_data = sar.Ortholog_Sequence_Dataset(orthologs, genome_datasets_dir, genome_ids, non_cds_offset, tb_species, single_copy = False) \n",
    "#print(outgroup_seq_data.species_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0047098f-893c-4544-987e-68471f522afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_data.generate_synteny_plot()\n",
    "seq_data.generate_ortholog_count_plot()\n",
    "#all_copy_seq_data.generate_master_count_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3b630-5c42-499e-8389-5532f4dca58c",
   "metadata": {},
   "source": [
    "##### Perform CDS and non-CDS alignments for each full ortholog group and save to folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb33f80-a608-425f-bf01-6a0fe7a77226",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_species = num_ids\n",
    "min_species_with_outgroup = num_ids_with_outgroup\n",
    "groups = random.sample(orthologs.full_single_copy_ortholog_groups, len(orthologs.full_single_copy_ortholog_groups))  #Permutation ensures even distribution of processing speeds\n",
    "outgroup_groups = random.sample(outgroup_orthologs.full_single_copy_ortholog_groups, len(outgroup_orthologs.full_single_copy_ortholog_groups))  #Permutation ensures even distribution of processing speeds\n",
    "if full_run == True:\n",
    "    par = Parallel(n_jobs=-1)(delayed(align.align_and_build)(outgroup_groups, num_cores, core_number, outgroup_seq_data.sequence_data, 'cds_length', 'cds_seq', outgroup_cds_output_dir+'/', min_species_with_outgroup) for core_number in tqdm(core_numbers))\n",
    "    par = Parallel(n_jobs=-1)(delayed(align.align_and_build)(groups, num_cores, core_number, seq_data.sequence_data, 'cds_length', 'cds_seq', cds_output_dir+'/', min_species) for core_number in tqdm(core_numbers))\n",
    "    par = Parallel(n_jobs=-1)(delayed(align.align_and_build)(groups, num_cores, core_number, seq_data.sequence_data, 'non_cds_offset_length', 'non_cds_offset_seq', non_cds_output_dir+'/', min_species) for core_number in tqdm(core_numbers))\n",
    "    par = Parallel(n_jobs=-1)(delayed(align.align_and_build)(groups, num_cores, core_number, seq_data.sequence_data, 'upstream_non_cds_offset_length', 'upstream_non_cds_offset_seq', upstream_non_cds_output_dir+'/', min_species) for core_number in tqdm(core_numbers))\n",
    "    par = Parallel(n_jobs=-1)(delayed(align.align_and_build)(groups, num_cores, core_number, seq_data.sequence_data, 'cds_extended_region_length', 'cds_extended_region_seq', extended_cds_output_dir+'/', min_species) for core_number in tqdm(core_numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552a5b9-a396-450f-956b-0494d74e13af",
   "metadata": {},
   "source": [
    "##### Run IQTree on concatenated CDS alignments to generate tree and rename with full names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42928a48-444f-4b46-a8eb-d668dd4565e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    alignment_names = util.list_files(outgroup_cds_output_dir)\n",
    "    util.concatenate_fasta(outgroup_cds_output_dir, alignment_names, outgroup_concatenated_cds_output_dir + '/concatenated_cds.fasta')\n",
    "    subprocess.run('cd \\\\users\\\\nicho\\\\IQTree & bin\\\\iqtree2 -s ' + outgroup_concatenated_cds_output_dir + '/concatenated_cds.fasta' + ' --prefix '+ output_dir + \n",
    "                   '/Trees/Concatenated_JC_Tree -m JC -B 1000 -T AUTO -o ' + outgroup_species, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1da0b-5a83-47c4-aa6a-77ae862dd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    organism_names = outgroup_seq_data.sequence_data[['species','name']].drop_duplicates().reset_index(drop=True)\n",
    "    organism_dict = {}\n",
    "    for i, r in organism_names.iterrows():\n",
    "        organism_dict[r['species']] = r['name'].split(' ')[0][0] + '.'+r['name'].split(' ')[1]\n",
    "        master_tree = ete3.Tree(output_dir + '/Trees/Concatenated_JC_Tree.treefile')\n",
    "    for node in master_tree.traverse():\n",
    "        if node.is_leaf():\n",
    "            node.name = organism_dict[node.name] \n",
    "    master_tree.write(format=0, outfile= output_dir + '/Trees/Concatenated_JC_Tree_Full_Names.treefile')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40396ae4-b80d-4ebd-ab66-e40e6712c758",
   "metadata": {},
   "source": [
    "##### Fit overall Alignment and Pairwise HMMs using EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e885b08-1828-4633-9c60-4eeaab22efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_symbols = 4\n",
    "num_states = 3\n",
    "minimum_fit_length = 10\n",
    "initial_params = [0.95, 0.5, 0.95, 0.5, 0.95, 0.5, 0.56370018, 0.52131172, 0.33906948]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da4636-b1db-4d25-a846-72c8fc4190a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Alignment_HMM_Model = alignment_hmm.Alignment_HMM (num_symbols, num_states, non_cds_output_dir, tb_species)\n",
    "if full_run == True:\n",
    "    parameter_fits = Alignment_HMM_Model.EM_update(num_cores, initial_params, non_cds_offset, minimum_fit_length)\n",
    "    fitted_parameters = parameter_fits[3]\n",
    "    with open(hmm_parameters_output_dir + '/' + 'full_parameter_fit.pkl', 'wb') as f:\n",
    "        pickle.dump(fitted_parameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1b4b3-11cc-4e5a-aea4-3d29f0810066",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    parameter_fits = []\n",
    "    for id in non_target_genome_ids:\n",
    "        parameter_fits.append((id, Alignment_HMM_Model.EM_update(num_cores, initial_params, non_cds_offset, minimum_fit_length, all_species = False, comparison_species = id)[3]))\n",
    "    with open(hmm_parameters_output_dir + '/' + 'pairwise_parameter_fits.pkl', 'wb') as f:\n",
    "        pickle.dump(parameter_fits, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251d95b-1df6-4bc2-8302-52d5c498ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Max likelihood using general optimisation - slower than EM\n",
    "#bound_tuple = [(0.001,0.999),(0.001,0.999),(0.001,0.999),(0.001,0.999),(0.001,0.999),(0.001,0.999),(0.001,0.999),(0.001,0.999),(0.001,0.999)]\n",
    "#def parallel_alignment_hmm_log_likelihood (params):\n",
    "#    core_numbers = range(1, num_cores+1)\n",
    "#    a = Parallel(n_jobs=-1)(delayed(Alignment_HMM_Model.alignment_hmm_log_likelihood)(params, num_cores, core_number, non_cds_offset, minimum_fit_length) for core_number in core_numbers)\n",
    "#    print(params, sum(a))\n",
    "#    return sum(a)  \n",
    "#    res = opt.minimize(parallel_alignment_hmm_log_likelihood, params, method = 'Nelder-Mead', bounds = bound_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75965597-303a-442b-9c61-2f2d1420e1b0",
   "metadata": {},
   "source": [
    "##### Fit overall HMM to pairwise HMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55473b24-e661-4d7f-a9df-e4a485723fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_generate_pairwise_state_probabilities(num_subsets, subset_num, ids, num_states, pairwise_fitted_parameters):\n",
    "    ids = util.chunk_list(ids, num_subsets, subset_num)\n",
    "    pairwise_observation_probabilities = []\n",
    "    for group_id in ids:\n",
    "        temp = []\n",
    "        alignment = align.Alignment(non_cds_output_dir +'/'+str(group_id)+'.fasta', tb_species, 'NT')\n",
    "        alignment.modify_sequence(1,False,False)\n",
    "        for params in pairwise_fitted_parameters:\n",
    "                transition_probabilities, mutation_probabilities = Alignment_HMM_Model.alignment_hmm_model_inputs(params[1])\n",
    "                observation_probabilities = Alignment_HMM_Model.calculate_observation_probs(mutation_probabilities, alignment.modified_sequence_list, alignment, all_species=False, comparison_species = params[0])\n",
    "                initial_state_probabilities = [1.0/num_states]*num_states\n",
    "                hmm_model = hmm.HMM(initial_state_probabilities, transition_probabilities, observation_probabilities)\n",
    "                hmm_model.calculate_probabilities()\n",
    "                temp.append(hmm_model.state_probabilities)\n",
    "        pairwise_observation_probabilities.append(temp) \n",
    "    return pairwise_observation_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5182ac-6442-40fc-9286-c32fcb173955",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hmm_parameters_output_dir + '/' + 'pairwise_parameter_fits.pkl', 'rb') as f:\n",
    "    pairwise_fitted_parameters = pickle.load(f)\n",
    "file_ids = util.list_files(non_cds_output_dir + '/')\n",
    "ids = [int(i.split('.')[0]) for i in file_ids] \n",
    "parallel_output = Parallel(n_jobs=-1)(delayed(parallel_generate_pairwise_state_probabilities)(num_cores, core_number, ids, num_states, pairwise_fitted_parameters) for core_number in core_numbers)\n",
    "pairwise_observation_probabilities = [item for sublist in parallel_output for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892c503c-dec6-4473-aa3b-38b41ed2b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Master_Alignment_HMM_Model = master_alignment_hmm.Master_Alignment_HMM (pairwise_observation_probabilities)\n",
    "initial_params = [0.8, 0.5, 0.9, 0.3]\n",
    "if full_run == True:\n",
    "    parameter_fits = Master_Alignment_HMM_Model.EM_update(num_cores, initial_params, non_cds_offset, minimum_fit_length)\n",
    "    fitted_parameters = parameter_fits[3]\n",
    "    print(fitted_parameters)\n",
    "    with open(hmm_parameters_output_dir + '/' + 'master_parameter_fit.pkl', 'wb') as f:\n",
    "        pickle.dump(fitted_parameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea8afe-116b-4ee5-9b77-f5e3fed532da",
   "metadata": {},
   "source": [
    "##### Load HMM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea7f2a-cd8b-4d52-bf7b-f3b8bde0536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hmm_parameters_output_dir + '/' + 'full_parameter_fit.pkl', 'rb') as f:\n",
    "    full_fitted_parameters = pickle.load(f)\n",
    "with open(hmm_parameters_output_dir + '/' + 'pairwise_parameter_fits.pkl', 'rb') as f:\n",
    "    pairwise_fitted_parameters = pickle.load(f)\n",
    "with open(hmm_parameters_output_dir + '/' + 'master_parameter_fit.pkl', 'rb') as f:\n",
    "    master_fitted_parameters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d794a82-bb9b-4718-ba24-a0402f9ab137",
   "metadata": {},
   "source": [
    "##### Analyse ortholog groups for conservation and other features and output to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507652cd-375b-4fe5-a06b-ae1491be7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_build_analysis_dictionary(num_subsets, subset_num, ids, analysis_type):\n",
    "    ids = util.chunk_list(ids, num_subsets, subset_num)\n",
    "    output_list = []\n",
    "    for group_id in ids:\n",
    "        alignment = align.Alignment(alignment_dir+'/'+str(group_id)+'.fasta', tb_species, 'NT')\n",
    "        analysis = alignment_analysis.Alignment_Analysis(analysis_type, alignment, num_states, non_cds_offset, group_id, full_fitted_parameters, project_dir, Alignment_HMM_Model, \n",
    "                                                         Master_Alignment_HMM_Model, pairwise_fitted_parameters, master_fitted_parameters, seq_data)\n",
    "        output_list.append((group_id, analysis))\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f2946-1234-4fb8-acdd-e936b8995dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    for analysis_type in ['Downstream', 'Upstream']:\n",
    "        if analysis_type == 'Downstream':\n",
    "            alignment_dir = non_cds_output_dir\n",
    "            dict_name = 'downstream_conservation_info_dictionary'\n",
    "        else:\n",
    "            alignment_dir = upstream_non_cds_output_dir\n",
    "            dict_name = 'upstream_conservation_info_dictionary'\n",
    "        alignment_info_dict = {}\n",
    "        file_ids = util.list_files(alignment_dir+'/')\n",
    "        ids = [int(i.split('.')[0]) for i in file_ids]\n",
    "        parallel_output = Parallel(n_jobs=-1)(delayed(parallel_build_analysis_dictionary)(num_cores, core_number, ids, analysis_type) for core_number in tqdm(core_numbers))\n",
    "        dictionary_list = [item for sublist in parallel_output for item in sublist]\n",
    "        alignment_info_dict = {}\n",
    "        for (group, analysis) in dictionary_list:\n",
    "            alignment_info_dict[group] = analysis\n",
    "        with open(conservation_analysis_output_dir + '/' + dict_name + '.pkl', 'wb') as f:\n",
    "            pickle.dump(alignment_info_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890e0c5-f1e1-4a94-b2e5-5ddccc4aac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(conservation_analysis_output_dir + '/upstream_conservation_info_dictionary.pkl', 'rb') as f:\n",
    "    upstream_conservation_info_dictionary = pickle.load(f)\n",
    "with open(conservation_analysis_output_dir + '/downstream_conservation_info_dictionary.pkl', 'rb') as f:\n",
    "    downstream_conservation_info_dictionary = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d610d-9e76-4cbe-be84-188b7ec12515",
   "metadata": {},
   "source": [
    "##### Plot graphics to show sequence and HMM regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddfa341-6dd7-4c7a-8974-5d9c6fdd8de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id = 1169 #1337 1007     #use 1120 in slides!  Also 1115 is interesting to discuss\n",
    "     #1120 can see the labels work!\n",
    "    #1161 1115 1116 758 1337   1525?List of sRNA in    \n",
    "    #1129\n",
    "    #1169 shows upstream start in DeJesus\n",
    "    #2131 not very well conserved in Arnvig\n",
    "    #1214 massive!\n",
    "    #1009 not much conservation\n",
    "    # 993 definitely looks like an upstream start site for reverse strand??\n",
    "upstream_conservation_info_dictionary[group_id].display_analysis()\n",
    "downstream_conservation_info_dictionary[group_id].display_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71cd470-a934-4d48-b3b5-9fc7c2c6161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = upstream_conservation_info_dictionary[group_id]\n",
    "#plt.plot(data.alignment.relative_entropy);\n",
    "plt.plot(data.alignment.mvave_relative_entropy);\n",
    "plt.axvline(x=data.buffer_end, ymin=0, ymax=2, color='r');\n",
    "plt.axvline(x=data.target_end, ymin=0, ymax=2, color='r');\n",
    "for state in [0]:\n",
    "    plt.plot(data.hmm_model.state_probabilities[state]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e8b96-1a28-4cbd-9325-7f77bdad0c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp= seq_data.sequence_data\n",
    "group_id = (temp[temp['locus_tag'] == 'Rv3504'].iloc[0]['group_id'])\n",
    "temp[temp['group_id'] == group_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5025afd-8475-4060-b664-bef60ef272e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
