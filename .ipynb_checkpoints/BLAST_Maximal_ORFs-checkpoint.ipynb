{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef24ce4-4729-44b6-8d43-fbdf47b9033a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Set up packages and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05aa9444-1c5a-4aa9-857a-4c640c89d19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e471e9d2-ea1c-4bbb-b2a7-9e8d7ff9e003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.fft import fft, fftfreq\n",
    "from Bio import Entrez, SeqIO, AlignIO, pairwise2, Align, Seq, motifs\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation\n",
    "from scipy.stats import binom\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from Comparative_Analysis import Sequence_Analysis_Routines as sar\n",
    "from Comparative_Analysis import Utilities as util\n",
    "from Comparative_Analysis import Alignment as align\n",
    "from Comparative_Analysis import ORF_Functions as orffn\n",
    "import random\n",
    "import copy\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import subprocess\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "from Bio.Align.Applications import MuscleCommandline\n",
    "muscle_exe = 'C:/Users/nicho/Muscle/muscle3.8.31_i86win32.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1988a242-f558-4643-b3a9-27272fa7bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = 'F:/Project_Data/Project_10'\n",
    "literature_datasets_dir = 'F:/Datasets/Data_From_Publications'\n",
    "output_dir = project_dir + '/Output'\n",
    "refseq_dir = 'F:/Datasets/NCBI_Refseq_Mycobacteriaceae_All_Levels/ncbi_dataset/data'\n",
    "num_cores = 8\n",
    "core_numbers = list(range(1, num_cores+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abef54a8-1fd0-4a05-94de-bc9e2f647844",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = util.list_dirs(refseq_dir)\n",
    "reference_species = 'GCF_000195955.2'\n",
    "species_list_excl_ref = [x for x in species_list if x!= reference_species]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "204fe3e6-c484-4ff8-9bd1-e5ac1915dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in SeqIO.parse(refseq_dir + '/'+reference_species+'/genomic.gbff', \"genbank\"):\n",
    "    full_sequence = str(record.seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b7740fa-b88a-4be0-b936-fea7d4122261",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_nts = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c20e948c-31e8-4060-ae6d-6e116453889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('F:/Project_Data/Project_11/Thoth_Full_Run/zero_and_non_zero_mutation_counts.pkl', 'rb') as f:\n",
    "    full_sample_zero_and_non_zero_mutation_counts = pickle.load(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8dc7d3d9-c30e-4c31-bb30-a481f33d23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_formula(position_3_counts, tot_bin_counts):\n",
    "    return 1- binom.cdf(position_3_counts-1, tot_bin_counts,1/3)\n",
    "def mutation_bin_probability(mutation_counts):\n",
    "    bin_counts = [0,0,0]\n",
    "    for i, c in enumerate(mutation_counts):\n",
    "        bin_counts[i % 3] += min(c,10000000)\n",
    "    if sum(bin_counts) == 0:\n",
    "        return (bin_counts, 2)\n",
    "    else:\n",
    "        return (bin_counts, bin_formula(bin_counts[2], sum(bin_counts)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7168f6bc-b147-4a2e-9390-5274fcb4ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_blast_file(record_list, output_filename):\n",
    "    with open(output_filename, 'w',  newline='') as outfile:\n",
    "        line_length = 60\n",
    "        for record in tqdm(record_list):\n",
    "            sequence = record[1]\n",
    "            lines = []\n",
    "            sequence_length = len(sequence)\n",
    "            number_of_lines = math.ceil(sequence_length / line_length)\n",
    "            lines.append(\">\" +record[0]+ \"\\n\")\n",
    "            for i in range(number_of_lines):\n",
    "                subsequence = sequence[i*line_length:(i+1)*line_length]\n",
    "                lines.append(subsequence + \"\\n\")\n",
    "            outfile.write(''.join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc97354-4eec-4741-b8b0-58b7be52aa9b",
   "metadata": {},
   "source": [
    "##### Function to find maximal open reading frame between two co-ordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23757841-ce5e-4eaa-9f83-c94f63b145a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_maximal_orfs(sequence, seq_start, seq_stop, output_all_orfs = False, min_orf_length = 0):\n",
    "    max_len = 0\n",
    "    orfs_found = []\n",
    "    start_pos = -999\n",
    "    end_pos = -999\n",
    "    for frame in ['Forward', 'Reverse']:\n",
    "        if frame == 'Forward':\n",
    "            temp = (sequence[seq_start: seq_stop])\n",
    "        else:\n",
    "            temp = align.reverse_complement(sequence[seq_start: seq_stop])\n",
    "        seq_len = len(temp)\n",
    "        for rf in range(3):\n",
    "            i = rf\n",
    "            while i < seq_len - 2:\n",
    "                orf_length = 0\n",
    "                test_codon = temp[i: i+3] \n",
    "                if test_codon in ['ATG','GTG','TTG']:  \n",
    "                    for j in range(i + 3, seq_len - 2, 3):\n",
    "                        test_codon_2 = temp[j: j+3] \n",
    "                        if test_codon_2 in ['TAG','TGA','TAA']:\n",
    "                            orf_length = j - i\n",
    "                            break\n",
    "                            \n",
    "                if orf_length > 0:\n",
    "                    if frame == 'Forward':\n",
    "                        orf_start =  seq_start + i\n",
    "                        orf_end = seq_start + j+3\n",
    "                        orf_strand = 1\n",
    "                    else:\n",
    "                        orf_start =  seq_start + seq_len-(j+3)\n",
    "                        orf_end = seq_start + seq_len-i\n",
    "                        orf_strand = -1\n",
    "                    \n",
    "                    if orf_length >= min_orf_length:\n",
    "                        orfs_found.append((orf_start, orf_end, orf_strand, orf_length))\n",
    "\n",
    "                if orf_length > max_len and orf_length >= min_orf_length:                                           \n",
    "                    max_len = orf_length\n",
    "                    start_pos = orf_start\n",
    "                    end_pos = orf_end\n",
    "                    strand = orf_strand \n",
    "\n",
    "                if orf_length > 0:\n",
    "                    i = j\n",
    "                else:\n",
    "                    i +=3\n",
    "    if output_all_orfs == True:\n",
    "        sorted_orfs = sorted(orfs_found, key=lambda x: x[3], reverse=True)\n",
    "        return sorted_orfs                \n",
    "    elif start_pos == -999:\n",
    "        return(0,0,0,0)\n",
    "    else:\n",
    "        return(start_pos, end_pos, strand, max_len)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d030d-93fc-42d5-9a5d-15c81d94f5a9",
   "metadata": {},
   "source": [
    "##### Function to find nearest larger and smaller ORFs enclosing region with same stop codon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50304da6-9a6a-4049-8652-9b2dd20cd51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_upstream_orf_sequence(sequence, seq_start, seq_stop, strand, max_lookback = 100):\n",
    "    out_seq = ''\n",
    "    if strand == 1:\n",
    "            temp = (sequence[seq_start - max_lookback * 3: seq_stop])\n",
    "    else:\n",
    "            temp = align.reverse_complement(sequence[seq_start: seq_stop + max_lookback * 3])\n",
    "    for lookback in range(max_lookback, -1, -1):\n",
    "        if temp[lookback * 3: (lookback+1) *3 ] in ['TAG','TGA','TAA']:\n",
    "            #  Not possible without first encountering stop - just return original sequence\n",
    "            out_seq = translate_sequence(temp[max_lookback * 3:],1,0)\n",
    "            break\n",
    "        if temp[lookback * 3: (lookback+1) *3 ] in ['ATG','GTG','TTG']:\n",
    "            out_seq = translate_sequence(temp[lookback * 3:],1,0)\n",
    "            break\n",
    "    return(out_seq)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6072f484-eaa0-4045-b08e-0a0cdc9e4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_downstream_orf_sequence(sequence, seq_start, seq_stop, strand):\n",
    "    max_lookforward = max(1, int((seq_stop - seq_start) / 3) - 4)\n",
    "    out_seq = ''\n",
    "    if strand == 1:\n",
    "            temp = sequence[seq_start:seq_stop]\n",
    "    else:\n",
    "            temp = align.reverse_complement(sequence[seq_start:seq_stop])\n",
    "    for lookback in range(max_lookforward):\n",
    "        if temp[lookback * 3: (lookback+1) *3 ] in ['TAG','TGA','TAA']:\n",
    "            #  Not possible without first encountering stop - just return original sequence\n",
    "            out_seq = translate_sequence(temp,1,0)\n",
    "            break\n",
    "        if temp[lookback * 3: (lookback+1) *3 ] in ['ATG','GTG','TTG']:\n",
    "            out_seq = translate_sequence(temp[lookback * 3:],1,0)\n",
    "            break\n",
    "    return(out_seq)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1fc097-47b4-4cf3-adda-8c19c8baabd2",
   "metadata": {},
   "source": [
    "#####  Function to process output from BLAST into dataframe with looked up values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f79c4637-0466-4dd8-8fed-5f36972c1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_blast_output(infile_loc, outfile_loc):\n",
    "    trans = util.Translator()\n",
    "    blast_results = pd.read_csv(infile_loc, header = None)\n",
    "    blast_results.columns = ['query_accession_ver', 'subject_accession_ver', 'query_length', 'subject_length', 'percent_identical_matches','alignment_length', 'number_mismatches', 'number_of_gap_openings', 'query_start_alignment', 'query_end_alignment', 'subject_start_alignment', 'subject_end_alignment', 'e_value', 'bit_score']\n",
    "    blast_results['query_info']=  blast_results['query_accession_ver'].map(protein_reference_dict)\n",
    "    blast_results['subject_info']=  blast_results['subject_accession_ver'].map(protein_reference_dict)\n",
    "    blast_results['query_feature']=  blast_results['query_accession_ver'].map(orf_feature_dict)\n",
    "    blast_results['subject_feature']=  blast_results['subject_accession_ver'].map(orf_feature_dict)\n",
    "    for i, r in blast_results.iterrows():\n",
    "        blast_results.at[i, 'query_species'] = '_'.join(r.query_accession_ver.split('_')[0:2])\n",
    "        blast_results.at[i, 'subject_species'] = '_'.join(r.subject_accession_ver.split('_')[0:2])\n",
    "    blast_results = blast_results.query('not (query_species == subject_species)')\n",
    "    blast_results['query_species_name'] = blast_results['query_species'].map(names_dict)\n",
    "    blast_results['subject_species_name'] = blast_results['subject_species'].map(names_dict)\n",
    "    for i, r in blast_results.iterrows():\n",
    "        if r.query_info[2] == 1:\n",
    "            blast_results.at[i, 'query_start_pos'] = r.query_info[0] + (r.query_start_alignment - 1) * 3\n",
    "            blast_results.at[i, 'query_end_pos'] = r.query_info[0] + (r.query_end_alignment - 1) * 3 + 6 # doesn't include stop codon in blast\n",
    "        else:\n",
    "            blast_results.at[i, 'query_start_pos'] = r.query_info[1] - (r.query_end_alignment) * 3\n",
    "            blast_results.at[i, 'query_end_pos'] = r.query_info[1] - (r.query_start_alignment - 1) * 3\n",
    "\n",
    "        if r.subject_info[2] == 1:\n",
    "            blast_results.at[i, 'subject_start_pos'] = r.subject_info[0] + (r.subject_start_alignment - 1) * 3\n",
    "            blast_results.at[i, 'subject_end_pos'] = r.subject_info[0] + (r.subject_end_alignment - 1) * 3 + 6 # doesn't include stop codon in blast\n",
    "        else:\n",
    "            blast_results.at[i, 'subject_start_pos'] = r.subject_info[1] - (r.subject_end_alignment) * 3\n",
    "            blast_results.at[i, 'subject_end_pos'] = r.subject_info[1] - (r.subject_start_alignment - 1) * 3\n",
    "    blast_results = blast_results.loc[blast_results.groupby(['query_accession_ver','subject_species'])['bit_score'].idxmax()]\n",
    "    blast_results['species_count'] = blast_results.groupby('query_accession_ver')['query_accession_ver'].transform('size')\n",
    "    for i, r in blast_results.iterrows():\n",
    "        subject_strand = r.subject_info[2]\n",
    "        #blast_results.at[i,'sequence'] = translate_orf(myco_info_dict[r.subject_species][1], int(r.subject_start_pos), int(r.subject_end_pos), subject_strand)\n",
    "        #blast_results.at[i,'nearest_upstream_orf_sequence'] = find_nearest_upstream_orf_sequence(myco_info_dict[r.subject_species][1], int(r.subject_start_pos), int(r.subject_end_pos), subject_strand, max_lookback = 100)\n",
    "        #blast_results.at[i,'nearest_downstream_orf_sequence'] = find_nearest_downstream_orf_sequence(myco_info_dict[r.subject_species][1], int(r.subject_start_pos), int(r.subject_end_pos), subject_strand)\n",
    "    with open(outfile_loc, 'wb') as f:\n",
    "        pickle.dump(blast_results, f)\n",
    "    return blast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9616c59-268c-4738-9682-0c2d4fa3cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_reciprocal_best_hits(query_df, reverse_query_df, outfile_loc):\n",
    "    temp_1_dict = {}\n",
    "    temp_2_dict = {}\n",
    "    for i, r in query_df.iterrows():\n",
    "        temp_1_dict[r['query_accession_ver']] = r['subject_accession_ver']\n",
    "    for i, r in reverse_query_df.iterrows():\n",
    "        temp_2_dict[r['query_accession_ver']] = r['subject_accession_ver']\n",
    "    for i, r in query_df.iterrows():\n",
    "        if temp_1_dict[r['query_accession_ver']] in temp_2_dict and temp_2_dict[temp_1_dict[r['query_accession_ver']]] == r['query_accession_ver']:\n",
    "            query_df.at[i, 'reciprocal_best_hit'] = 'Y'\n",
    "        else:\n",
    "            query_df.at[i, 'reciprocal_best_hit'] = 'N'\n",
    "    output = query_df[query_df.reciprocal_best_hit == 'Y'] \n",
    "    with open(outfile_loc, 'wb') as f:\n",
    "        pickle.dump(output, f)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ef9e8-e455-4ec3-b342-29749e142d86",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Extract full sequences from each organism and create directory of start and stops for each annotated cds (use Mycobrowser for MTb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8064f57-5199-4b73-afc3-c6d8676f91b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_myco_info(num_subsets, subset_num, species_master_list):\n",
    "    trans = util.Translator()\n",
    "    output = []\n",
    "    species_list = util.chunk_list(species_master_list, num_subsets, subset_num)\n",
    "    for species in species_list:\n",
    "        features = []\n",
    "        genome_record = next(SeqIO.parse(refseq_dir + '/'+species+'/genomic.gbff', \"genbank\"))\n",
    "        full_sequence = str(genome_record.seq)\n",
    "        if full_sequence.count('A') + full_sequence.count('C') + full_sequence.count('G') + full_sequence.count('T') < len(full_sequence):\n",
    "            continue\n",
    "        organism = genome_record.annotations['organism']\n",
    "        \n",
    "        #  Read feature information\n",
    "        if species == reference_species:\n",
    "            mycobrowser_df = pd.read_excel(literature_datasets_dir+'/Mycobrowser_Release_4.xlsx')\n",
    "            for i, r in mycobrowser_df.iterrows():\n",
    "                if r['Strand'] == '+':\n",
    "                    strand = 1\n",
    "                else:\n",
    "                    strand = -1\n",
    "                features.append((r['Locus'],r['Start']-1, r['Stop'], strand))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            for feature in genome_record.features:\n",
    "                    a = feature.qualifiers\n",
    "                    if feature.type == 'gene' and a.get(\"locus_tag\")!= None and int(feature.location.end) - int(feature.location.start) < 100000:  #  Exclude strange Biopython parsing where starts with complement join and looks like a CDS is full length of genome!   \n",
    "                        locus_tag = a.get(\"locus_tag\")[0]\n",
    "                        features.append([locus_tag, int(feature.location.start), int(feature.location.end), int(feature.location.strand)])\n",
    "        \n",
    "        features.sort(key=lambda x: x[1])\n",
    "        #  Find maximal orfs, non-overlapping orfs and their protein sequences, and assign a reference to each\n",
    "        maximal_orfs = find_all_maximal_orfs(full_sequence, 0, len(full_sequence), True, min_nts)\n",
    "        protein_references = []\n",
    "        maximal_orf_proteins = []\n",
    "        morf_feature_map = []\n",
    "        non_overlapping_maximal_orfs = []\n",
    "        non_overlapping_maximal_orf_proteins = []\n",
    "        non_overlapping_orfs = []\n",
    "        non_overlapping_orf_proteins = []\n",
    "        for i, orf in enumerate(maximal_orfs):\n",
    "            #translation = translate_orf(full_sequence, orf[0], orf[1], orf[2])[:-1]\n",
    "            translation = trans.translate_sequence(full_sequence[orf[0]:orf[1]], orf[2],0)[:-1]\n",
    "            morf_name = species+'_'+str(i)\n",
    "            maximal_orf_proteins.append([morf_name, translation])\n",
    "            protein_references.append((morf_name, orf))\n",
    "            overlap = False\n",
    "            for cds in features:\n",
    "                if ((orf[2] == 1 and orf[0] <= cds[1] and orf[1] == cds[2]) or (orf[2] == -1 and orf[0] == cds[1] and orf[1] >= cds[2])) and orf[2] == cds[3]:\n",
    "                    morf_feature_map.append((morf_name, cds[0]))\n",
    "            for cds in features:    \n",
    "                if min(cds[2], orf[1]) - max(cds[1], orf[0]) > 0.3 * (orf[1] - orf[0]):\n",
    "                    overlap = True\n",
    "                    break\n",
    "            if overlap == False:\n",
    "                nomorf_name = species+'_NOM_'+str(i)\n",
    "                non_overlapping_maximal_orfs.append(orf)\n",
    "                non_overlapping_maximal_orf_proteins.append([nomorf_name, translation])\n",
    "                protein_references.append((nomorf_name, orf))\n",
    "        \n",
    "        non_overlapping_features = []\n",
    "\n",
    "        for i, cds in enumerate(features):\n",
    "            if i > 0 and cds[2] > features[i-1][2]:\n",
    "                non_overlapping_features.append(cds)\n",
    "        for i, cds in enumerate(non_overlapping_features):\n",
    "            if i < len(non_overlapping_features) - 1:\n",
    "                temp = find_all_maximal_orfs(full_sequence, cds[2], non_overlapping_features[i+1][1], True, min_nts)\n",
    "                for orf in temp:\n",
    "                    non_overlapping_orfs.append(orf)\n",
    "        for i, orf in enumerate(non_overlapping_orfs):\n",
    "            #translation = translate_orf(full_sequence, orf[0], orf[1], orf[2])[:-1]\n",
    "            translation = trans.translate_sequence(full_sequence[orf[0]:orf[1]], orf[2],0)[:-1]\n",
    "            noorf_name = species+'_NO_'+str(i)\n",
    "            non_overlapping_orf_proteins.append([noorf_name, translation])\n",
    "            protein_references.append((noorf_name, orf))\n",
    "            \n",
    "        output.append((species, organism, full_sequence, features, maximal_orfs, non_overlapping_maximal_orfs, maximal_orf_proteins, non_overlapping_maximal_orf_proteins, protein_references, morf_feature_map, non_overlapping_orfs, non_overlapping_orf_proteins))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55a62613-b631-468a-90e9-ff3e0fe9255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "#if 1==1:\n",
    "    myco_info_dict = {}\n",
    "    names_dict = {}\n",
    "    parallel_output = Parallel(n_jobs=-1)(delayed(generate_myco_info)(num_cores, core_number, species_list) for core_number in core_numbers)\n",
    "    for core_output in parallel_output:\n",
    "        for results in core_output:\n",
    "            myco_info_dict[results[0]] = (results[1], results[2], results[3], results[4], results[5], results[6], results[7], results[8], results[9], results[10], results[11])\n",
    "            names_dict[results[0]] = results[1]\n",
    "    with open(project_dir + '/' + 'myco_info_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(myco_info_dict, f)\n",
    "    with open(project_dir + '/' + 'names_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(names_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "863423ec-2cdf-470c-928d-9f9ad1a6fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(full_run == True):\n",
    "    with open(project_dir + '/' + 'myco_info_dict.pkl', 'rb') as f:\n",
    "        myco_info_dict = pickle.load(f)    \n",
    "    with open(project_dir + '/' + 'names_dict.pkl', 'rb') as f:\n",
    "        names_dict = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7e099d0-f6f3-45d7-8d85-3c36d7ce3054",
   "metadata": {},
   "outputs": [],
   "source": [
    "orf_feature_dict = {}\n",
    "feature_morf_dict = {}\n",
    "protein_reference_dict = {}\n",
    "orf_protein_dict = {}\n",
    "for species in species_list:\n",
    "    if species in myco_info_dict:\n",
    "        for ref in myco_info_dict[species][7]:\n",
    "            protein_reference_dict[ref[0]] = ref[1]\n",
    "            orf_protein_dict[(species, ref[1])] = ref[0]\n",
    "        for ref in myco_info_dict[species][8]:\n",
    "            orf_feature_dict[ref[0]] = ref[1]\n",
    "            feature_morf_dict[ref[1]] = ref[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4046ad-c780-42ac-843e-375ce3d2e0aa",
   "metadata": {},
   "source": [
    "##### Output two blast files - one (subject) containing all translated mORFs, the other (query) just the ones for the reference species "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66a5ad12-f3b7-4a08-ba72-1960f22779a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    subject_protein_list = []\n",
    "    for species in species_list_excl_ref:\n",
    "        if species in myco_info_dict:\n",
    "            for maximal_orf_protein in myco_info_dict[species][5]:\n",
    "                subject_protein_list.append(maximal_orf_protein)\n",
    "    produce_blast_file(subject_protein_list, project_dir + '/subject_proteins.faa')\n",
    "\n",
    "    query_protein_list = myco_info_dict[reference_species][6]\n",
    "    produce_blast_file(query_protein_list, project_dir + '/no_overlap_morf_query_proteins.faa')\n",
    "\n",
    "    query_protein_list = myco_info_dict[reference_species][10]\n",
    "    produce_blast_file(query_protein_list, project_dir + '/no_overlap_orf_query_proteins.faa')\n",
    "\n",
    "    query_protein_list = myco_info_dict[reference_species][5]\n",
    "    produce_blast_file(query_protein_list, project_dir + '/morf_query_proteins.faa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5fe6a4-dcf0-4b12-9257-c73f35691298",
   "metadata": {},
   "source": [
    "##### Create blast databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cf8fedc-abfd-41b4-a82e-3ccaf8902e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    w_d = os.getcwd()\n",
    "    os.chdir(\"F:/\")\n",
    "    subprocess.run('cd '+ project_dir + ' &  makeblastdb -in subject_proteins.faa -dbtype prot -out subj_prot', shell=True, capture_output = True)\n",
    "    subprocess.run('cd '+ project_dir + ' &  makeblastdb -in morf_query_proteins.faa -dbtype prot -out query_prot', shell=True, capture_output = True)\n",
    "    subprocess.run('cd '+ project_dir + ' &  makeblastdb -in no_overlap_morf_query_proteins.faa -dbtype prot -out no_overlap_morf_query_prot', shell=True, capture_output = True)\n",
    "    subprocess.run('cd '+ project_dir + ' &  makeblastdb -in no_overlap_orf_query_proteins.faa -dbtype prot -out no_overlap_orf_query_prot', shell=True, capture_output = True)\n",
    "    os.chdir(w_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a80f40f-3343-4dd2-8fd8-c485077a1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  To do - copy database files into BLAST subfolders - currently done manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b1565b0-47b3-476f-888c-3cf5156e8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    w_d = os.getcwd()\n",
    "    os.chdir(\"F:/\")\n",
    "    subprocess.run('cd f:\\\\Datasets\\\\BLAST\\\\subj_prot & blastp -query morf_query_proteins.faa -db subj_prot -out blastp_results_query_subject.csv -evalue 1e-7 -seg no -outfmt  \"10 qaccver saccver qlen slen pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -num_threads 16', shell=True, capture_output = True)\n",
    "    subprocess.run('cd f:\\\\Datasets\\\\BLAST\\\\subj_prot & blastp -query no_overlap_morf_query_proteins.faa -db subj_prot -out blastp_results_no_morf_query_subject.csv -evalue 1e-7 -seg no -outfmt  \"10 qaccver saccver qlen slen pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -num_threads 16', shell=True, capture_output = True)\n",
    "    subprocess.run('cd f:\\\\Datasets\\\\BLAST\\\\subj_prot & blastp -query no_overlap_orf_query_proteins.faa -db subj_prot -out blastp_results_no_orf_query_subject.csv -evalue 1e-7 -seg no -outfmt  \"10 qaccver saccver qlen slen pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -num_threads 16', shell=True, capture_output = True)\n",
    "\n",
    "    subprocess.run('cd f:\\\\Datasets\\\\BLAST\\\\query_prot & blastp -query subject_proteins.faa -db query_prot -out blastp_results_subject_query.csv -evalue 1e-7 -seg no -outfmt  \"10 qaccver saccver qlen slen pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -num_threads 16', shell=True, capture_output = True)\n",
    "    subprocess.run('cd f:\\\\Datasets\\\\BLAST\\\\no_overlap_morf_query_prot & blastp -query subject_proteins.faa -db no_overlap_morf_query_prot -out blastp_results_subject_no_morf_query.csv -evalue 1e-7 -seg no -outfmt  \"10 qaccver saccver qlen slen pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -num_threads 16', shell=True, capture_output = True)\n",
    "    subprocess.run('cd f:\\\\Datasets\\\\BLAST\\\\no_overlap_orf_query_prot & blastp -query subject_proteins.faa -db no_overlap_orf_query_prot -out blastp_results_subject_no_orf_query.csv -evalue 1e-7 -seg no -outfmt  \"10 qaccver saccver qlen slen pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -num_threads 16', shell=True, capture_output = True)\n",
    "\n",
    "    os.chdir(w_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dafef7a7-f78b-43b8-afbb-15ffe2b6b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "#if 1==1:\n",
    "    blast_results_qs = process_blast_output('F:\\\\Datasets\\\\BLAST\\\\subj_prot\\\\blastp_results_query_subject.csv', project_dir + '/blast_results_qs.pkl')\n",
    "    blast_results_sq = process_blast_output('F:\\\\Datasets\\\\BLAST\\\\query_prot\\\\blastp_results_subject_query.csv', project_dir + '/blast_results_sq.pkl')\n",
    "    blast_results_nomqs = process_blast_output('F:\\\\Datasets\\\\BLAST\\\\subj_prot\\\\blastp_results_no_morf_query_subject.csv', project_dir + '/blast_results_nomqs.pkl')\n",
    "    blast_results_snomq = process_blast_output('F:\\\\Datasets\\\\BLAST\\\\no_overlap_morf_query_prot\\\\blastp_results_subject_no_morf_query.csv', project_dir + '/blast_results_snomq.pkl')\n",
    "    blast_results_nooqs = process_blast_output('F:\\\\Datasets\\\\BLAST\\\\subj_prot\\\\blastp_results_no_orf_query_subject.csv', project_dir + '/blast_results_nooqs.pkl')\n",
    "    blast_results_snooq = process_blast_output('F:\\\\Datasets\\\\BLAST\\\\no_overlap_orf_query_prot\\\\blastp_results_subject_no_orf_query.csv', project_dir + '/blast_results_snooq.pkl')\n",
    "    rbh_orf = keep_reciprocal_best_hits(blast_results_qs, blast_results_sq, project_dir + '/rbh_orf_results.pkl')\n",
    "    rbh_non_overlap_morf = keep_reciprocal_best_hits(blast_results_nomqs, blast_results_snomq, project_dir + '/rbh_non_overlap_morf_results.pkl')\n",
    "    rbh_non_overlap_orf = keep_reciprocal_best_hits(blast_results_nooqs, blast_results_snooq, project_dir + '/rbh_non_overlap_orf_results.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6482890-d697-473f-8779-2a4e89a41303",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(full_run == True):\n",
    "     with open(project_dir + '/' + 'rbh_non_overlap_morf_results.pkl', 'rb') as f:\n",
    "        rbh_non_overlap_morf = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669265b8-3432-45c8-8444-46736b63c080",
   "metadata": {},
   "source": [
    "#####   Filter promising candidates and record alternative start codons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "39c1067f-77ea-4dbf-b108-f6f445b264ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = util.Translator()\n",
    "temp_df = rbh_non_overlap_morf.query(\"abs(query_end_alignment - query_length) < 10 and abs(subject_end_alignment - subject_length) <10 and species_count > 5\")\n",
    "temp_df['species_count'] = temp_df.groupby('query_accession_ver')['query_accession_ver'].transform('size')\n",
    "temp_df = temp_df.query(\"species_count > 10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "22da3e74-0d4d-4e63-b66b-f0b7dd616bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_codon_positions_dict = {}\n",
    "for i, r in temp_df.iterrows():\n",
    "    (start, stop, strand, length) = r['query_info']\n",
    "    a = trans.translate_sequence(full_sequence[start:stop], strand, 0, True, True)\n",
    "    positions = []\n",
    "    for n, x in enumerate(a):\n",
    "        if x == 'Z':\n",
    "            positions.append(n+1)   # Make 1 based in line with BLAST\n",
    "    start_codon_positions_dict[r['query_accession_ver']] = positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0d80e02c-5ab0-41f6-8d93-dc0cbd8e82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "genome_record = next(SeqIO.parse(project_dir + '/annot.gbk', \"genbank\"))\n",
    "for feature in genome_record.features:\n",
    "        a = feature.qualifiers\n",
    "        if feature.type == 'CDS' and a.get(\"locus_tag\")!= None and int(feature.location.end) - int(feature.location.start) < 100000:  #  Exclude strange Biopython parsing where starts with complement join and looks like a CDS is full length of genome!   \n",
    "            locus_tag = a.get(\"locus_tag\")[0]\n",
    "            if a.get(\"product\") != None:\n",
    "                product = a.get(\"product\")[0]\n",
    "            else:\n",
    "                product = ''\n",
    "            features.append([locus_tag, product, int(feature.location.start), int(feature.location.end), int(feature.location.strand)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "494d9965-c868-47ef-8be9-01e3205963c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotation_features_dict = {}\n",
    "for (locus_tag, product, start, end, strand) in features:\n",
    "    if strand == 1:\n",
    "        new_annotation_features_dict[end] = (locus_tag, product, start, end, strand)\n",
    "    else:\n",
    "        new_annotation_features_dict[start] = (locus_tag, product, start, end, strand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2c7e2c97-9e40-4e29-acea-c63ec59f7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in temp_df.iterrows():\n",
    "    (start, stop, strand, length) = r['query_info']\n",
    "    if strand == 1:\n",
    "        temp = stop\n",
    "        prob = mutation_bin_probability(full_sample_zero_and_non_zero_mutation_counts[start+3:stop-3])\n",
    "    else:\n",
    "        temp = start\n",
    "        prob = mutation_bin_probability(reversed(full_sample_zero_and_non_zero_mutation_counts[start+3:stop-3]))\n",
    "    if temp in new_annotation_features_dict:\n",
    "        temp_df.at[i,'product'] = new_annotation_features_dict[temp][1]\n",
    "    else:\n",
    "        temp_df.at[i,'product'] = 'Unmatched'\n",
    "    temp_df.at[i, 'prob'] = prob[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "67d9d417-8e70-48e7-a9e2-6540428abb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df['target_start_codon_positions']=  temp_df['query_accession_ver'].map(start_codon_positions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d170ce11-60ed-4533-8c17-326810d48ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.to_csv(project_dir + '/temp_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8a65fdfb-8c8c-4095-8f1d-484f12b05006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_df['query_accession_ver'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b94d9-0ae1-4a63-8191-72f270a3394f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
