{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef24ce4-4729-44b6-8d43-fbdf47b9033a",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Set up packages and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05aa9444-1c5a-4aa9-857a-4c640c89d19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e471e9d2-ea1c-4bbb-b2a7-9e8d7ff9e003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.fft import fft, fftfreq\n",
    "from Bio import Entrez, SeqIO, AlignIO, pairwise2, Align, Seq, motifs\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation\n",
    "from scipy.stats import binom\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from Comparative_Analysis import Sequence_Analysis_Routines as sar\n",
    "from Comparative_Analysis import Utilities as util\n",
    "from Comparative_Analysis import Alignment as align\n",
    "from Comparative_Analysis import ORF_Functions as orffn\n",
    "import random\n",
    "import copy\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import subprocess\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "from Bio.Align.Applications import MuscleCommandline\n",
    "muscle_exe = 'C:/Users/nicho/Muscle/muscle3.8.31_i86win32.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1988a242-f558-4643-b3a9-27272fa7bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = 'F:/Project_Data/Project_10'\n",
    "literature_datasets_dir = 'F:/Datasets/Data_From_Publications'\n",
    "output_dir = project_dir + '/Output'\n",
    "refseq_dir = 'F:/Datasets/NCBI_Refseq_Mycobacteriaceae_All_Levels/ncbi_dataset/data'\n",
    "num_cores = 8\n",
    "core_numbers = list(range(1, num_cores+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abef54a8-1fd0-4a05-94de-bc9e2f647844",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = util.list_dirs(refseq_dir)\n",
    "reference_species = 'GCF_000195955.2'\n",
    "species_list_excl_ref = [x for x in species_list if x!= reference_species]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c35235a-c07b-4f48-88a8-eaa211790eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_nts = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab243605-4807-411c-8195-401b1a404c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_blast_file(record_list, output_filename):\n",
    "    with open(output_filename, 'w',  newline='') as outfile:\n",
    "        line_length = 60\n",
    "        for record in tqdm(record_list):\n",
    "            sequence = record[1]\n",
    "            lines = []\n",
    "            sequence_length = len(sequence)\n",
    "            number_of_lines = math.ceil(sequence_length / line_length)\n",
    "            lines.append(\">\" +record[0]+ \"\\n\")\n",
    "            for i in range(number_of_lines):\n",
    "                subsequence = sequence[i*line_length:(i+1)*line_length]\n",
    "                lines.append(subsequence + \"\\n\")\n",
    "            outfile.write(''.join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc97354-4eec-4741-b8b0-58b7be52aa9b",
   "metadata": {},
   "source": [
    "##### Function to find maximal open reading frame between two co-ordinates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23757841-ce5e-4eaa-9f83-c94f63b145a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_maximal_orfs(sequence, seq_start, seq_stop, output_all_orfs = False, min_orf_length = 0):\n",
    "    max_len = 0\n",
    "    orfs_found = []\n",
    "    start_pos = -999\n",
    "    end_pos = -999\n",
    "    for frame in ['Forward', 'Reverse']:\n",
    "        if frame == 'Forward':\n",
    "            temp = (sequence[seq_start: seq_stop])\n",
    "        else:\n",
    "            temp = align.reverse_complement(sequence[seq_start: seq_stop])\n",
    "        seq_len = len(temp)\n",
    "        for rf in range(3):\n",
    "            i = rf\n",
    "            while i < seq_len - 2:\n",
    "                orf_length = 0\n",
    "                test_codon = temp[i: i+3] \n",
    "                if test_codon in ['ATG','GTG','TTG']:  \n",
    "                    for j in range(i + 3, seq_len - 2, 3):\n",
    "                        test_codon_2 = temp[j: j+3] \n",
    "                        if test_codon_2 in ['TAG','TGA','TAA']:\n",
    "                            orf_length = j - i\n",
    "                            break\n",
    "                            \n",
    "                if orf_length > 0:\n",
    "                    if frame == 'Forward':\n",
    "                        orf_start =  seq_start + i\n",
    "                        orf_end = seq_start + j+3\n",
    "                        orf_strand = 1\n",
    "                    else:\n",
    "                        orf_start =  seq_start + seq_len-(j+3)\n",
    "                        orf_end = seq_start + seq_len-i\n",
    "                        orf_strand = -1\n",
    "                    \n",
    "                    if orf_length >= min_orf_length:\n",
    "                        orfs_found.append((orf_start, orf_end, orf_strand, orf_length))\n",
    "\n",
    "                if orf_length > max_len and orf_length >= min_orf_length:                                           \n",
    "                    max_len = orf_length\n",
    "                    start_pos = orf_start\n",
    "                    end_pos = orf_end\n",
    "                    strand = orf_strand \n",
    "\n",
    "                if orf_length > 0:\n",
    "                    i = j\n",
    "                else:\n",
    "                    i +=3\n",
    "    if output_all_orfs == True:\n",
    "        sorted_orfs = sorted(orfs_found, key=lambda x: x[3], reverse=True)\n",
    "        return sorted_orfs                \n",
    "    elif start_pos == -999:\n",
    "        return(0,0,0,0)\n",
    "    else:\n",
    "        return(start_pos, end_pos, strand, max_len)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d030d-93fc-42d5-9a5d-15c81d94f5a9",
   "metadata": {},
   "source": [
    "##### Function to find nearest upstream start to an alignment making it an open reading frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50304da6-9a6a-4049-8652-9b2dd20cd51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_upstream_orf_sequence(sequence, seq_start, seq_stop, strand, max_lookback = 100):\n",
    "    out_seq = ''\n",
    "    if strand == 1:\n",
    "            temp = (sequence[seq_start - max_lookback * 3: seq_stop])\n",
    "    else:\n",
    "            temp = align.reverse_complement(sequence[seq_start: seq_stop + max_lookback * 3])\n",
    "    for lookback in range(max_lookback, -1, -1):\n",
    "        if temp[lookback * 3: (lookback+1) *3 ] in ['TAG','TGA','TAA']:\n",
    "            #  Not possible without first encountering stop - just return original sequence\n",
    "            out_seq = translate_sequence(temp[max_lookback * 3:],1,0)\n",
    "            break\n",
    "        if temp[lookback * 3: (lookback+1) *3 ] in ['ATG','GTG','TTG']:\n",
    "            out_seq = translate_sequence(temp[lookback * 3:],1,0)\n",
    "            break\n",
    "    return(out_seq)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072f484-eaa0-4045-b08e-0a0cdc9e4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_downstream_orf_sequence(sequence, seq_start, seq_stop, strand):\n",
    "    max_lookforward = max(1, int((seq_stop - seq_start) / 3) - 4)\n",
    "    out_seq = ''\n",
    "    if strand == 1:\n",
    "            temp = sequence[seq_start:seq_stop]\n",
    "    else:\n",
    "            temp = align.reverse_complement(sequence[seq_start:seq_stop])\n",
    "    for lookback in range(max_lookforward):\n",
    "        if temp[lookback * 3: (lookback+1) *3 ] in ['TAG','TGA','TAA']:\n",
    "            #  Not possible without first encountering stop - just return original sequence\n",
    "            out_seq = translate_sequence(temp,1,0)\n",
    "            break\n",
    "        if temp[lookback * 3: (lookback+1) *3 ] in ['ATG','GTG','TTG']:\n",
    "            out_seq = translate_sequence(temp[lookback * 3:],1,0)\n",
    "            break\n",
    "    return(out_seq)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1fc097-47b4-4cf3-adda-8c19c8baabd2",
   "metadata": {},
   "source": [
    "#####  Function to process output from BLAST into dataframe with looked up values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79c4637-0466-4dd8-8fed-5f36972c1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_blast_output(infile_loc, outfile_loc):\n",
    "    trans = util.Translator()\n",
    "    blast_results = pd.read_csv(infile_loc, header = None)\n",
    "    blast_results.columns = ['query_accession_ver', 'subject_accession_ver', 'query_length', 'subject_length', 'percent_identical_matches','alignment_length', 'number_mismatches', 'number_of_gap_openings', 'query_start_alignment', 'query_end_alignment', 'subject_start_alignment', 'subject_end_alignment', 'e_value', 'bit_score']\n",
    "    blast_results['query_info']=  blast_results['query_accession_ver'].map(protein_reference_dict)\n",
    "    blast_results['subject_info']=  blast_results['subject_accession_ver'].map(protein_reference_dict)\n",
    "    blast_results['query_feature']=  blast_results['query_accession_ver'].map(orf_feature_dict)\n",
    "    blast_results['subject_feature']=  blast_results['subject_accession_ver'].map(orf_feature_dict)\n",
    "    for i, r in blast_results.iterrows():\n",
    "        blast_results.at[i, 'query_species'] = '_'.join(r.query_accession_ver.split('_')[0:2])\n",
    "        blast_results.at[i, 'subject_species'] = '_'.join(r.subject_accession_ver.split('_')[0:2])\n",
    "    blast_results = blast_results.query('not (query_species == subject_species)')\n",
    "    blast_results['query_species_name'] = blast_results['query_species'].map(names_dict)\n",
    "    blast_results['subject_species_name'] = blast_results['subject_species'].map(names_dict)\n",
    "    for i, r in blast_results.iterrows():\n",
    "        if r.query_info[2] == 1:\n",
    "            blast_results.at[i, 'query_start_pos'] = r.query_info[0] + (r.query_start_alignment - 1) * 3\n",
    "            blast_results.at[i, 'query_end_pos'] = r.query_info[0] + (r.query_end_alignment - 1) * 3 + 6 # doesn't include stop codon in blast\n",
    "        else:\n",
    "            blast_results.at[i, 'query_start_pos'] = r.query_info[1] - (r.query_end_alignment) * 3\n",
    "            blast_results.at[i, 'query_end_pos'] = r.query_info[1] - (r.query_start_alignment - 1) * 3\n",
    "\n",
    "        if r.subject_info[2] == 1:\n",
    "            blast_results.at[i, 'subject_start_pos'] = r.subject_info[0] + (r.subject_start_alignment - 1) * 3\n",
    "            blast_results.at[i, 'subject_end_pos'] = r.subject_info[0] + (r.subject_end_alignment - 1) * 3 + 6 # doesn't include stop codon in blast\n",
    "        else:\n",
    "            blast_results.at[i, 'subject_start_pos'] = r.subject_info[1] - (r.subject_end_alignment) * 3\n",
    "            blast_results.at[i, 'subject_end_pos'] = r.subject_info[1] - (r.subject_start_alignment - 1) * 3\n",
    "    blast_results = blast_results.loc[blast_results.groupby(['query_accession_ver','subject_species'])['bit_score'].idxmax()]\n",
    "    blast_results['species_count'] = blast_results.groupby('query_accession_ver')['query_accession_ver'].transform('size')\n",
    "    for i, r in blast_results.iterrows():\n",
    "        subject_strand = r.subject_info[2]\n",
    "        #blast_results.at[i,'sequence'] = translate_orf(myco_info_dict[r.subject_species][1], int(r.subject_start_pos), int(r.subject_end_pos), subject_strand)\n",
    "        #blast_results.at[i,'nearest_upstream_orf_sequence'] = find_nearest_upstream_orf_sequence(myco_info_dict[r.subject_species][1], int(r.subject_start_pos), int(r.subject_end_pos), subject_strand, max_lookback = 100)\n",
    "        #blast_results.at[i,'nearest_downstream_orf_sequence'] = find_nearest_downstream_orf_sequence(myco_info_dict[r.subject_species][1], int(r.subject_start_pos), int(r.subject_end_pos), subject_strand)\n",
    "    with open(outfile_loc, 'wb') as f:\n",
    "        pickle.dump(blast_results, f)\n",
    "    return blast_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9616c59-268c-4738-9682-0c2d4fa3cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_reciprocal_best_hits(query_df, reverse_query_df, outfile_loc):\n",
    "    temp_1_dict = {}\n",
    "    temp_2_dict = {}\n",
    "    for i, r in query_df.iterrows():\n",
    "        temp_1_dict[r['query_accession_ver']] = r['subject_accession_ver']\n",
    "    for i, r in reverse_query_df.iterrows():\n",
    "        temp_2_dict[r['query_accession_ver']] = r['subject_accession_ver']\n",
    "    for i, r in query_df.iterrows():\n",
    "        if temp_1_dict[r['query_accession_ver']] in temp_2_dict and temp_2_dict[temp_1_dict[r['query_accession_ver']]] == r['query_accession_ver']:\n",
    "            query_df.at[i, 'reciprocal_best_hit'] = 'Y'\n",
    "        else:\n",
    "            query_df.at[i, 'reciprocal_best_hit'] = 'N'\n",
    "    output = query_df[query_df.reciprocal_best_hit == 'Y'] \n",
    "    with open(outfile_loc, 'wb') as f:\n",
    "        pickle.dump(output, f)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075ef9e8-e455-4ec3-b342-29749e142d86",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Extract full sequences from each organism and create directory of start and stops for each annotated cds (use Mycobrowser for MTb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8064f57-5199-4b73-afc3-c6d8676f91b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_myco_info(num_subsets, subset_num, species_master_list):\n",
    "    trans = util.Translator()\n",
    "    output = []\n",
    "    species_list = util.chunk_list(species_master_list, num_subsets, subset_num)\n",
    "    for species in species_list:\n",
    "        features = []\n",
    "        genome_record = next(SeqIO.parse(refseq_dir + '/'+species+'/genomic.gbff', \"genbank\"))\n",
    "        full_sequence = str(genome_record.seq)\n",
    "        if full_sequence.count('A') + full_sequence.count('C') + full_sequence.count('G') + full_sequence.count('T') < len(full_sequence):\n",
    "            continue\n",
    "        organism = genome_record.annotations['organism']\n",
    "        \n",
    "        #  Read feature information\n",
    "        if species == reference_species:\n",
    "            mycobrowser_df = pd.read_excel(literature_datasets_dir+'/Mycobrowser_Release_4.xlsx')\n",
    "            for i, r in mycobrowser_df.iterrows():\n",
    "                if r['Strand'] == '+':\n",
    "                    strand = 1\n",
    "                else:\n",
    "                    strand = -1\n",
    "                features.append((r['Locus'],r['Start']-1, r['Stop'], strand))\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            for feature in genome_record.features:\n",
    "                    a = feature.qualifiers\n",
    "                    if feature.type == 'gene' and a.get(\"locus_tag\")!= None and int(feature.location.end) - int(feature.location.start) < 100000:  #  Exclude strange Biopython parsing where starts with complement join and looks like a CDS is full length of genome!   \n",
    "                        locus_tag = a.get(\"locus_tag\")[0]\n",
    "                        features.append([locus_tag, int(feature.location.start), int(feature.location.end), int(feature.location.strand)])\n",
    "        \n",
    "        features.sort(key=lambda x: x[1])\n",
    "        #  Find maximal orfs, non-overlapping orfs and their protein sequences, and assign a reference to each\n",
    "        maximal_orfs = find_all_maximal_orfs(full_sequence, 0, len(full_sequence), True, min_nts)\n",
    "        protein_references = []\n",
    "        maximal_orf_proteins = []\n",
    "        morf_feature_map = []\n",
    "        non_overlapping_maximal_orfs = []\n",
    "        non_overlapping_maximal_orf_proteins = []\n",
    "        non_overlapping_orfs = []\n",
    "        non_overlapping_orf_proteins = []\n",
    "        for i, orf in enumerate(maximal_orfs):\n",
    "            #translation = translate_orf(full_sequence, orf[0], orf[1], orf[2])[:-1]\n",
    "            translation = trans.translate_sequence(full_sequence[orf[0]:orf[1]], orf[2],0)[:-1]\n",
    "            morf_name = species+'_'+str(i)\n",
    "            maximal_orf_proteins.append([morf_name, translation])\n",
    "            protein_references.append((morf_name, orf))\n",
    "            overlap = False\n",
    "            for cds in features:\n",
    "                if ((orf[2] == 1 and orf[0] <= cds[1] and orf[1] == cds[2]) or (orf[2] == -1 and orf[0] == cds[1] and orf[1] >= cds[2])) and orf[2] == cds[3]:\n",
    "                    morf_feature_map.append((morf_name, cds[0]))\n",
    "            for cds in features:    \n",
    "                if min(cds[2], orf[1]) - max(cds[1], orf[0]) > 0.3 * (orf[1] - orf[0]):\n",
    "                    overlap = True\n",
    "                    break\n",
    "            if overlap == False:\n",
    "                nomorf_name = species+'_NOM_'+str(i)\n",
    "                non_overlapping_maximal_orfs.append(orf)\n",
    "                non_overlapping_maximal_orf_proteins.append([nomorf_name, translation])\n",
    "                protein_references.append((nomorf_name, orf))\n",
    "        \n",
    "        non_overlapping_features = []\n",
    "\n",
    "        for i, cds in enumerate(features):\n",
    "            if i > 0 and cds[2] > features[i-1][2]:\n",
    "                non_overlapping_features.append(cds)\n",
    "        for i, cds in enumerate(non_overlapping_features):\n",
    "            if i < len(non_overlapping_features) - 1:\n",
    "                temp = find_all_maximal_orfs(full_sequence, cds[2], non_overlapping_features[i+1][1], True, min_nts)\n",
    "                for orf in temp:\n",
    "                    non_overlapping_orfs.append(orf)\n",
    "        for i, orf in enumerate(non_overlapping_orfs):\n",
    "            #translation = translate_orf(full_sequence, orf[0], orf[1], orf[2])[:-1]\n",
    "            translation = trans.translate_sequence(full_sequence[orf[0]:orf[1]], orf[2],0)[:-1]\n",
    "            noorf_name = species+'_NO_'+str(i)\n",
    "            non_overlapping_orf_proteins.append([noorf_name, translation])\n",
    "            protein_references.append((noorf_name, orf))\n",
    "            \n",
    "        output.append((species, organism, full_sequence, features, maximal_orfs, non_overlapping_maximal_orfs, maximal_orf_proteins, non_overlapping_maximal_orf_proteins, protein_references, morf_feature_map, non_overlapping_orfs, non_overlapping_orf_proteins))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a62613-b631-468a-90e9-ff3e0fe9255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "#if 1==1:\n",
    "    myco_info_dict = {}\n",
    "    names_dict = {}\n",
    "    parallel_output = Parallel(n_jobs=-1)(delayed(generate_myco_info)(num_cores, core_number, species_list) for core_number in core_numbers)\n",
    "    for core_output in parallel_output:\n",
    "        for results in core_output:\n",
    "            myco_info_dict[results[0]] = (results[1], results[2], results[3], results[4], results[5], results[6], results[7], results[8], results[9], results[10], results[11])\n",
    "            names_dict[results[0]] = results[1]\n",
    "    with open(project_dir + '/' + 'myco_info_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(myco_info_dict, f)\n",
    "    with open(project_dir + '/' + 'names_dict.pkl', 'wb') as f:\n",
    "        pickle.dump(names_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863423ec-2cdf-470c-928d-9f9ad1a6fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(full_run == True):\n",
    "    with open(project_dir + '/' + 'myco_info_dict.pkl', 'rb') as f:\n",
    "        myco_info_dict = pickle.load(f)    \n",
    "    with open(project_dir + '/' + 'names_dict.pkl', 'rb') as f:\n",
    "        names_dict = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7e099d0-f6f3-45d7-8d85-3c36d7ce3054",
   "metadata": {},
   "outputs": [],
   "source": [
    "orf_feature_dict = {}\n",
    "feature_morf_dict = {}\n",
    "protein_reference_dict = {}\n",
    "orf_protein_dict = {}\n",
    "for species in species_list:\n",
    "    if species in myco_info_dict:\n",
    "        for ref in myco_info_dict[species][7]:\n",
    "            protein_reference_dict[ref[0]] = ref[1]\n",
    "            orf_protein_dict[(species, ref[1])] = ref[0]\n",
    "        for ref in myco_info_dict[species][8]:\n",
    "            orf_feature_dict[ref[0]] = ref[1]\n",
    "            feature_morf_dict[ref[1]] = ref[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4046ad-c780-42ac-843e-375ce3d2e0aa",
   "metadata": {},
   "source": [
    "##### Output two blast files - one (subject) containing all translated mORFs, the other (query) just the ones for the reference species "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5ad12-f3b7-4a08-ba72-1960f22779a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    subject_protein_list = []\n",
    "    for species in species_list_excl_ref:\n",
    "        if species in myco_info_dict:\n",
    "            for maximal_orf_protein in myco_info_dict[species][5]:\n",
    "                subject_protein_list.append(maximal_orf_protein)\n",
    "    produce_blast_file(subject_protein_list, project_dir + '/subject_proteins.faa')\n",
    "\n",
    "    query_protein_list = myco_info_dict[reference_species][6]\n",
    "    produce_blast_file(query_protein_list, project_dir + '/no_overlap_morf_query_proteins.faa')\n",
    "\n",
    "    query_protein_list = myco_info_dict[reference_species][10]\n",
    "    produce_blast_file(query_protein_list, project_dir + '/no_overlap_orf_query_proteins.faa')\n",
    "\n",
    "    query_protein_list = myco_info_dict[reference_species][5]\n",
    "    produce_blast_file(query_protein_list, project_dir + '/morf_query_proteins.faa')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5fe6a4-dcf0-4b12-9257-c73f35691298",
   "metadata": {},
   "source": [
    "##### Create blast databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf8fedc-abfd-41b4-a82e-3ccaf8902e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    w_d = os.getcwd()\n",
    "    os.chdir(\"F:/\")\n",
    "    subprocess.run('cd '+ project_dir + ' &  makeblastdb -in subject_proteins.faa -dbtype prot -out subj_prot', shell=True, capture_output = True)\n",
    "    subprocess.run('cd '+ project_dir + ' &  makeblastdb -in morf_query_proteins.faa -dbtype prot -out query_prot', shell=True, capture_output = True)\n",
    "    subprocess.run('cd '+ project_dir + ' &  makeblastdb -in no_overlap_morf_query_proteins.faa -dbtype prot -out no_overlap_morf_query_prot', shell=True, capture_output = True)\n",
    "    subprocess.run('cd '+ project_dir + ' &  makeblastdb -in no_overlap_orf_query_proteins.faa -dbtype prot -out no_overlap_orf_query_prot', shell=True, capture_output = True)\n",
    "    os.chdir(w_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a80f40f-3343-4dd2-8fd8-c485077a1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  To do - copy database files into BLAST subfolders - currently done manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1565b0-47b3-476f-888c-3cf5156e8dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    w_d = os.getcwd()\n",
    "    os.chdir(\"F:/\")\n",
    "    subprocess.run('cd f:\\\\Datasets\\\\BLAST\\\\subj_prot & blastp -query morf_query_proteins.faa -db subj_prot -out blastp_results_query_subject.csv -evalue 1e-7 -seg no -outfmt  \"10 qaccver saccver qlen slen pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -num_threads 16', shell=True, capture_output = True)\n",
    "    subprocess.run('cd f:\\\\Datasets\\\\BLAST\\\\subj_prot & blastp -query no_overlap_morf_query_proteins.faa -db subj_prot -out blastp_results_no_morf_query_subject.csv -evalue 1e-7 -seg no -outfmt  \"10 qaccver saccver qlen slen pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -num_threads 16', shell=True, capture_output = True)\n",
    "    subprocess.run('cd f:\\\\Datasets\\\\BLAST\\\\subj_prot & blastp -query no_overlap_orf_query_proteins.faa -db subj_prot -out blastp_results_no_orf_query_subject.csv -evalue 1e-7 -seg no -outfmt  \"10 qaccver saccver qlen slen pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -num_threads 16', shell=True, capture_output = True)\n",
    "\n",
    "    subprocess.run('cd f:\\\\Datasets\\\\BLAST\\\\query_prot & blastp -query subject_proteins.faa -db query_prot -out blastp_results_subject_query.csv -evalue 1e-7 -seg no -outfmt  \"10 qaccver saccver qlen slen pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -num_threads 16', shell=True, capture_output = True)\n",
    "    subprocess.run('cd f:\\\\Datasets\\\\BLAST\\\\no_overlap_morf_query_prot & blastp -query subject_proteins.faa -db no_overlap_morf_query_prot -out blastp_results_subject_no_morf_query.csv -evalue 1e-7 -seg no -outfmt  \"10 qaccver saccver qlen slen pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -num_threads 16', shell=True, capture_output = True)\n",
    "    subprocess.run('cd f:\\\\Datasets\\\\BLAST\\\\no_overlap_orf_query_prot & blastp -query subject_proteins.faa -db no_overlap_orf_query_prot -out blastp_results_subject_no_orf_query.csv -evalue 1e-7 -seg no -outfmt  \"10 qaccver saccver qlen slen pident length mismatch gapopen qstart qend sstart send evalue bitscore\" -num_threads 16', shell=True, capture_output = True)\n",
    "\n",
    "    os.chdir(w_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafef7a7-f78b-43b8-afbb-15ffe2b6b3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "#if 1==1:\n",
    "    blast_results_qs = process_blast_output('F:\\\\Datasets\\\\BLAST\\\\subj_prot\\\\blastp_results_query_subject.csv', project_dir + '/blast_results_qs.pkl')\n",
    "    blast_results_sq = process_blast_output('F:\\\\Datasets\\\\BLAST\\\\query_prot\\\\blastp_results_subject_query.csv', project_dir + '/blast_results_sq.pkl')\n",
    "    blast_results_nomqs = process_blast_output('F:\\\\Datasets\\\\BLAST\\\\subj_prot\\\\blastp_results_no_morf_query_subject.csv', project_dir + '/blast_results_nomqs.pkl')\n",
    "    blast_results_snomq = process_blast_output('F:\\\\Datasets\\\\BLAST\\\\no_overlap_morf_query_prot\\\\blastp_results_subject_no_morf_query.csv', project_dir + '/blast_results_snomq.pkl')\n",
    "    blast_results_nooqs = process_blast_output('F:\\\\Datasets\\\\BLAST\\\\subj_prot\\\\blastp_results_no_orf_query_subject.csv', project_dir + '/blast_results_nooqs.pkl')\n",
    "    blast_results_snooq = process_blast_output('F:\\\\Datasets\\\\BLAST\\\\no_overlap_orf_query_prot\\\\blastp_results_subject_no_orf_query.csv', project_dir + '/blast_results_snooq.pkl')\n",
    "    rbh_orf = keep_reciprocal_best_hits(blast_results_qs, blast_results_sq, project_dir + '/rbh_orf_results.pkl')\n",
    "    rbh_non_overlap_morf = keep_reciprocal_best_hits(blast_results_nomqs, blast_results_snomq, project_dir + '/rbh_non_overlap_morf_results.pkl')\n",
    "    rbh_non_overlap_orf = keep_reciprocal_best_hits(blast_results_nooqs, blast_results_snooq, project_dir + '/rbh_non_overlap_orf_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dd1fe9-84a7-4029-b7ff-7595da70df55",
   "metadata": {},
   "source": [
    "#####  Count orthologs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef489a74-c80d-4c03-b410-af3eb051993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (full_run == True):\n",
    "    with open(project_dir + '/rbh_orf_results.pkl', 'rb') as f:\n",
    "        rbh_orf = pickle.load(f)  \n",
    "    with open(project_dir + '/rbh_non_overlap_morf_results.pkl', 'rb') as f:\n",
    "        rbh_non_overlap_morf = pickle.load(f) \n",
    "    with open(project_dir + '/rbh_non_overlap_orf_results.pkl', 'rb') as f:\n",
    "        rbh_non_overlap_orf = pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ac522-42c2-474d-a537-6d37e6809927",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbh_non_overlap_morf.to_csv(project_dir + '/rbh_non_overlap_morf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa0292-3f41-4e20-bea7-2f10410d2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = blast_results_qs.query('not(query_feature.isnull()) and not(subject_feature.isnull())')\n",
    "#temp.groupby('subject_species_name').count().to_csv(project_dir + '/one_way_ortholog_counts.csv')\n",
    "temp = rbh_orf.query('not(query_feature.isnull()) and not(subject_feature.isnull())')\n",
    "temp.groupby('subject_species_name').count().to_csv(project_dir + '/reciprocal_ortholog_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75beff10-eec3-4771-bc15-b221d68bfc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9dc17b-4a83-4fba-b53a-822de90097c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(names_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855cbef6-7941-4468-a97c-3cdf64803bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_list = []\n",
    "for species in species_list_excl_ref:\n",
    "    if species in names_dict:\n",
    "        temp_list.append([names_dict[species], len(myco_info_dict[species][2])])\n",
    "temp = pd.DataFrame(temp_list, columns = ['name','count'])\n",
    "temp = temp.sort_values('name')\n",
    "temp.to_csv(project_dir +  '/gene_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b01ee-4a62-4dcc-8ba0-a9ff87ed0dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = rbh_orf.query('not(query_feature.isnull()) and not(subject_feature.isnull()) and e_value < 1e-5')[['query_accession_ver','species_count']].drop_duplicates()\n",
    "print(len(temp))\n",
    "sns.histplot(data = temp, x = 'species_count', bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273677dc-b0bb-4d19-9343-c0491e1a83ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbh_orf.query('not(query_feature.isnull()) and not(subject_feature.isnull()) and e_value < 1e-5')[['subject_species_name', 'query_species_name','query_feature','subject_feature']].to_csv(project_dir + '/nick.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acfa28e-0932-4aad-bde5-a76b8c115cb8",
   "metadata": {},
   "source": [
    "#####  Identify commonly preserved mORFs in non-overlapping areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db559e8-1d3c-4c30-a1eb-5e0e9986a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_feature_dict = {}\n",
    "for feature in myco_info_dict[reference_species][2]:\n",
    "    ref_feature_dict[feature[0]] = [feature[1], feature[2], feature[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812311db-4009-4422-87d6-2a1bcc08ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = myco_info_dict[reference_species][2]\n",
    "feature_morfs = []\n",
    "for feature in feature_list:\n",
    "    locus_id = feature[0]\n",
    "    if not(locus_id in feature_morf_dict):\n",
    "        continue\n",
    "    feature_morfs.append(feature_morf_dict[locus_id])\n",
    "feature_morf_info = rbh_orf[['query_accession_ver','subject_accession_ver','query_species','subject_species','query_info','subject_info','species_count','subject_species_name','query_species_name','query_feature']]\n",
    "feature_morf_info = feature_morf_info.query('query_accession_ver in @feature_morfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541b137a-41b6-412a-8fc4-609d0980faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_consensus_start_sites(num_subsets, subset_num, feature_list):\n",
    "    features = util.chunk_list(feature_list, num_subsets, subset_num)\n",
    "    temp = feature_morf_info\n",
    "    output_list = []\n",
    "    for feature in features:\n",
    "        locus_id = feature[0]\n",
    "        if not(locus_id in feature_morf_dict):\n",
    "            continue\n",
    "        morf_id = feature_morf_dict[locus_id]\n",
    "        temp_df = temp[temp['query_accession_ver'] == morf_id]\n",
    "        with open(project_dir+'/testseq_'+str(subset_num)+'.fasta', 'w',  newline='') as outfile:\n",
    "            line_length = 60\n",
    "            for i, r in temp_df.iterrows():\n",
    "                sequence = translate_orf(myco_info_dict[r['subject_species']][1], r['subject_info'][0], r['subject_info'][1], r['subject_info'][2], True, True)\n",
    "                lines = []\n",
    "                sequence_length = len(sequence)\n",
    "                number_of_lines = math.ceil(sequence_length / line_length)\n",
    "                lines.append(\">\" +r['subject_species_name']+ \"\\n\")\n",
    "                for i in range(number_of_lines):\n",
    "                    subsequence = sequence[i*line_length:(i+1)*line_length]\n",
    "                    lines.append(subsequence + \"\\n\")\n",
    "                outfile.write(''.join(lines))\n",
    "            temp_df = temp_df.head(1)\n",
    "            for i, r in temp_df.iterrows():\n",
    "                sequence = translate_orf(myco_info_dict[r['query_species']][1], r['query_info'][0], r['query_info'][1], r['query_info'][2], True, True)\n",
    "                lines = []\n",
    "                sequence_length = len(sequence)\n",
    "                number_of_lines = math.ceil(sequence_length / line_length)\n",
    "                lines.append(\">\" +r['query_species_name']+ \"\\n\")\n",
    "                for i in range(number_of_lines):\n",
    "                    subsequence = sequence[i*line_length:(i+1)*line_length]\n",
    "                    lines.append(subsequence + \"\\n\")\n",
    "                outfile.write(''.join(lines))\n",
    "        cline = MuscleCommandline(muscle_exe, input=project_dir+'/testseq_'+str(subset_num)+'.fasta', out=project_dir+'/testalign_'+str(subset_num)+'.fasta')\n",
    "        try:\n",
    "            stdout, stderr = cline()\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        alignment = util.read_fasta_to_array(project_dir+'/testalign_'+str(subset_num)+'.fasta')\n",
    "        for i, species_name in enumerate(alignment[0]):\n",
    "            if species_name == names_dict[reference_species]:\n",
    "                reference_pos = i\n",
    "        locus_name = orf_feature_dict[morf_id]\n",
    "        feature_coordinates = ref_feature_dict[locus_name]\n",
    "        morf_coordinates = protein_reference_dict[morf_id]\n",
    "        if feature_coordinates[2] == 1:\n",
    "            annotated_start_offset = (feature_coordinates[0] - morf_coordinates[0])/3\n",
    "        else:\n",
    "            annotated_start_offset = (morf_coordinates[1] - feature_coordinates[1])/3\n",
    "\n",
    "        start_codon_counts = []\n",
    "        consensus_start_codon_positions = []\n",
    "        num_species = len(alignment[0])\n",
    "        for i in range(len(alignment[1][1])):\n",
    "            aligned_column = [x[i] for x in alignment[1]]\n",
    "            if not(aligned_column[reference_pos] == '-'):\n",
    "                if aligned_column[reference_pos] == 'Z':\n",
    "                    column_start_count = aligned_column.count('Z')\n",
    "                else:\n",
    "                    column_start_count = 0\n",
    "                start_codon_counts.append(column_start_count)\n",
    "        for i, ct in enumerate(start_codon_counts):\n",
    "            if ct > 0.6*num_species:\n",
    "                consensus_start_codon_positions.append(i - annotated_start_offset)\n",
    "        #print(morf_id, locus_name, feature_coordinates, morf_coordinates, annotated_start_offset, consensus_start_codon_positions)  \n",
    "        output_list.append((morf_id, locus_name, feature_coordinates, morf_coordinates, annotated_start_offset, consensus_start_codon_positions))\n",
    "        #print(start_codon_counts)\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeca53e-7ec6-411a-b79e-758c413ba012",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "feature_list = myco_info_dict[reference_species][2]\n",
    "parallel_output = Parallel(n_jobs=-1)(delayed(generate_consensus_start_sites)(num_cores, core_number, feature_list) for core_number in core_numbers)\n",
    "for core_output in parallel_output:\n",
    "    for results in core_output:\n",
    "        temp.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca36578-dfcb-4999-a450-b4451fc645aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =pd.DataFrame(temp, columns = ['morf_id','locus','locus_coords','morf_cords','annotated_start_offset','consensus_start_codon_positions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f013a-345e-4e05-be05-075ad9570c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv(project_dir + '/start_sites.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa96fb-2e78-48e7-922b-08c019491b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = temp, x = 'species_count', bins = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7b1c3-6716-40f3-9326-2207c378b610",
   "metadata": {},
   "source": [
    "##### Investigate conservation of start positions in orthologs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72320c1c-a3ec-488d-bba4-53b6b727da18",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_orf_dict = {}\n",
    "for k, v in orf_feature_dict.items():\n",
    "    feature_orf_dict[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a960ead-2431-4961-8306-945585c1f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "close_species_list = ['GCF_000009445.1', 'GCF_000157895.3','GCF_000195955.2','GCF_000253375.1', 'GCF_000157895.3', 'GCF_003584745.1', 'GCF_003614435.1', 'GCF_010728525.1', 'GCF_010730055.1' , 'GCF_010731535.1', 'GCF_013466425.1', 'GCF_016745295.1', 'GCF_018278905.1', 'GCF_900292015.1', 'LT_708304.1']\n",
    "close_species_df = rbh_orf.query('subject_species in @close_species_list')\n",
    "for i, feature in enumerate(myco_info_dict[reference_species][2]):\n",
    "    if i<100:\n",
    "        continue\n",
    "    if i > 200:\n",
    "        break\n",
    "    strand = feature[3]\n",
    "    name = feature[0]\n",
    "    if strand == 1:\n",
    "        start = feature[1]\n",
    "    else:\n",
    "        start = feature[2]\n",
    "        \n",
    "    if not (feature[0] in feature_orf_dict):\n",
    "        pass\n",
    "    else:\n",
    "        df = close_species_df.query('query_feature == @feature[0]')[['query_feature', 'query_start_pos', 'query_end_pos']]\n",
    "        if len(df) > 5:\n",
    "            if strand == 1:\n",
    "                df['offset'] = (df['query_start_pos'] - start)/3 \n",
    "            else:\n",
    "                df['offset'] = (start - df['query_end_pos'])/3\n",
    "            df_list.append(df)\n",
    "temp = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905f0e0-6492-4b59-9a2b-9d86b5a62cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.query('offset > -80 and offset < 80')\n",
    "plt.figure(figsize=(5, 5))\n",
    "g = sns.FacetGrid(temp, col='query_feature', height=2, col_wrap=8)\n",
    "g.map(sns.histplot, 'offset', bins=12)\n",
    "g.set_titles(row_template = '{row_name}', col_template = '{col_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f3790-d2c1-4863-bb3f-433308803747",
   "metadata": {},
   "source": [
    "##### Violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b90381-2875-4d69-8e48-647297c3a704",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[temp['query_feature'] == 'Rv0003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f05446-a0bc-46ab-878f-6f49731b58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "g = sns.FacetGrid(rbh_orf, col='subject_species_name', height=5, col_wrap=3)\n",
    "g.map(sns.scatterplot, 'subject_start_pos', 'query_start_pos', s=2)\n",
    "g.set_titles(row_template = '{row_name}', col_template = '{col_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc24677-4372-406c-9b54-dca2d2ed1e48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mutation_bin_probability' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24988/4146037367.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmutation_bin_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1282029\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1282218\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mutation_bin_probability' is not defined"
     ]
    }
   ],
   "source": [
    "mutation_bin_probability(1282029, 1282218,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb95f1-dd24-4ab4-ae28-9f24ea2cf703",
   "metadata": {},
   "source": [
    "#### Output possibly conserved ORF regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec796a-b52c-42d0-ad61-8f835825bad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = copy.deepcopy(rbh_orf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb2ff6-6094-497b-940d-7d00692a1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2 = temp[['query_accession_ver','species_count','query_start_pos', 'query_end_pos','query_info','query_feature','e_value']]\n",
    "for i, r in temp_2.iterrows():\n",
    "    temp_2.at[i,'query_len'] = r['query_end_pos'] -  r['query_start_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd2da30-ee93-4415-ac9b-c055fa5794ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2['median_length'] = temp_2.groupby('query_accession_ver')['query_len'].transform('median')\n",
    "temp_2['median_start'] = temp_2.groupby('query_accession_ver')['query_start_pos'].transform('median')\n",
    "temp_2['median_end'] = temp_2.groupby('query_accession_ver')['query_end_pos'].transform('median')\n",
    "temp_2['mean_e_value'] = temp_2.groupby('query_accession_ver')['e_value'].transform('mean')\n",
    "temp_2 = temp_2[['query_accession_ver','species_count','query_info','query_feature','median_length', 'median_start','median_end','mean_e_value']]\n",
    "temp_2 = temp_2.drop_duplicates()\n",
    "temp_2 = temp_2.sort_values(by = ['median_length','species_count'], ascending = [False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbda64-eb35-43cf-9ac9-510da793589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference_coverage(start_pos, end_pos):\n",
    "    features = []\n",
    "    non_overlapped_features = []\n",
    "    for feature in myco_info_dict[reference_species][2]:\n",
    "        if feature[1] <= end_pos and feature[2] >= start_pos:\n",
    "            features.append((max(feature[1], start_pos), min(feature[2], end_pos)))\n",
    "    features.sort(key=lambda x: x[0])\n",
    "    features_len = len(features)\n",
    "    for i, feature in enumerate(features):\n",
    "        if i == features_len - 1:\n",
    "            non_overlapped_features.append(feature)\n",
    "        else:\n",
    "            if feature[1] > features[i+1][0]:\n",
    "                non_overlapped_features.append((feature[0], features[i+1][0]))\n",
    "            else:\n",
    "                non_overlapped_features.append(feature)\n",
    "    coverage = 0\n",
    "    for (x, y) in non_overlapped_features:\n",
    "        coverage += (y-x)\n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a8f46-7313-4afa-9446-23c49e058f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in temp_2.iterrows():\n",
    "    temp_2.at[i, 'coverage'] = reference_coverage(r['median_start'], r['median_end'])\n",
    "temp_2.to_csv(project_dir + '/nick.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47c523-af41-471a-bdde-bafb0f220b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "nick = temp_2.query('median_start < 13650 & median_end > 13700')\n",
    "nick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b8501-7322-4950-bc85-c7af01b0cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "myco_info_dict[reference_species][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881459ec-c1d4-4ec1-a03f-de39e8de4cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_coverage(4255900,4255960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0853840-cd87-4f16-88e3-b790057cdee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = rbh_orf\n",
    "output_list = []\n",
    "subset_num = 1\n",
    "locus_id = 'Rv3801c'\n",
    "offset = 0\n",
    "morf_id = feature_morf_dict[locus_id]\n",
    "\n",
    "temp_df = temp[temp['query_accession_ver'] == morf_id]\n",
    "with open(project_dir+'/testseq_'+str(subset_num)+'.fasta', 'w',  newline='') as outfile:\n",
    "    line_length = 60\n",
    "    for i, r in temp_df.iterrows():\n",
    "        if r['subject_info'][2] == 1:\n",
    "            sequence = translate_orf(myco_info_dict[r['subject_species']][1], r['subject_info'][0], r['subject_info'][1]+offset, r['subject_info'][2], True, True)\n",
    "        else:\n",
    "            sequence = translate_orf(myco_info_dict[r['subject_species']][1], r['subject_info'][0]-offset, r['subject_info'][1], r['subject_info'][2], True, True)\n",
    "        lines = []\n",
    "        sequence_length = len(sequence)\n",
    "        number_of_lines = math.ceil(sequence_length / line_length)\n",
    "        lines.append(\">\" +r['subject_species_name']+ \"\\n\")\n",
    "        for i in range(number_of_lines):\n",
    "            subsequence = sequence[i*line_length:(i+1)*line_length]\n",
    "            lines.append(subsequence + \"\\n\")\n",
    "        outfile.write(''.join(lines))\n",
    "    temp_df = temp_df.head(1)\n",
    "    for i, r in temp_df.iterrows():\n",
    "        if r['query_info'][2] == 1:\n",
    "            sequence = translate_orf(myco_info_dict[r['query_species']][1], r['query_info'][0], r['query_info'][1]+offset, r['query_info'][2], True, True)\n",
    "        else:\n",
    "            sequence = translate_orf(myco_info_dict[r['query_species']][1], r['query_info'][0]-offset, r['query_info'][1], r['query_info'][2], True, True)\n",
    "        lines = []\n",
    "        sequence_length = len(sequence)\n",
    "        number_of_lines = math.ceil(sequence_length / line_length)\n",
    "        lines.append(\">\" +r['query_species_name']+ \"\\n\")\n",
    "        for i in range(number_of_lines):\n",
    "            subsequence = sequence[i*line_length:(i+1)*line_length]\n",
    "            lines.append(subsequence + \"\\n\")\n",
    "        outfile.write(''.join(lines))\n",
    "cline = MuscleCommandline(muscle_exe, input=project_dir+'/testseq_'+str(subset_num)+'.fasta', out=project_dir+'/testalign_'+str(subset_num)+'.fasta')\n",
    "cline()\n",
    "alignment = util.read_fasta_to_array(project_dir+'/testalign_'+str(subset_num)+'.fasta')\n",
    "\n",
    "\n",
    "# for i, species_name in enumerate(alignment[0]):\n",
    "#     if species_name == names_dict[reference_species]:\n",
    "#         reference_pos = i\n",
    "# locus_name = orf_feature_dict[morf_id]\n",
    "# feature_coordinates = ref_feature_dict[locus_name]\n",
    "# morf_coordinates = protein_reference_dict[morf_id]\n",
    "# if feature_coordinates[2] == 1:\n",
    "#     annotated_start_offset = (feature_coordinates[0] - morf_coordinates[0])/3\n",
    "# else:\n",
    "#     annotated_start_offset = (morf_coordinates[1] - feature_coordinates[1])/3\n",
    "\n",
    "# start_codon_counts = []\n",
    "# consensus_start_codon_positions = []\n",
    "# num_species = len(alignment[0])\n",
    "# for i in range(len(alignment[1][1])):\n",
    "#     aligned_column = [x[i] for x in alignment[1]]\n",
    "#     if not(aligned_column[reference_pos] == '-'):\n",
    "#         if aligned_column[reference_pos] == 'Z':\n",
    "#             column_start_count = aligned_column.count('Z')\n",
    "#         else:\n",
    "#             column_start_count = 0\n",
    "#         start_codon_counts.append(column_start_count)\n",
    "# for i, ct in enumerate(start_codon_counts):\n",
    "#     if ct > 0.6*num_species:\n",
    "#         consensus_start_codon_positions.append(i - annotated_start_offset)\n",
    "# print(morf_id, locus_name, feature_coordinates, morf_coordinates, annotated_start_offset, consensus_start_codon_positions)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6482890-d697-473f-8779-2a4e89a41303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
