{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50da8fa4-e4d1-4e99-9df9-f2593cc0c057",
   "metadata": {},
   "source": [
    "##### Import modules and set up file locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9417e01-aa2a-4297-8948-c0c526434b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_run = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "694b2495-3118-4d6a-9934-5f46d80dfa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicho\\Anaconda3\\envs\\Bioinformatics\\lib\\site-packages\\ete3-3.1.2-py3.7.egg\\ete3\\evol\\parser\\codemlparser.py:221: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\nicho\\Anaconda3\\envs\\Bioinformatics\\lib\\site-packages\\ete3-3.1.2-py3.7.egg\\ete3\\evol\\parser\\codemlparser.py:221: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"
     ]
    }
   ],
   "source": [
    "from Comparative_Analysis import Sequence_Analysis_Routines as sar\n",
    "from Comparative_Analysis import HMM as hmm\n",
    "from Comparative_Analysis import Utilities as util\n",
    "from Comparative_Analysis import Alignment_HMM as alignment_hmm\n",
    "from Comparative_Analysis import Alignment_Analysis as alignment_analysis\n",
    "from Comparative_Analysis import Alignment as align\n",
    "from Comparative_Analysis import Master_Alignment_HMM as master_alignment_hmm\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import optimize as opt\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import logomaker as lm\n",
    "import math\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import ete3;\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "773a1d9c-8b02-447d-8b4c-41e9d5d24f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = 'D:/Project_Data/Project_4'\n",
    "genome_datasets_dir = project_dir + '/Datasets/NCBI_Datasets'\n",
    "output_dir = project_dir + '/Output'\n",
    "protein_fasta_output_loc = output_dir + '/Protein_Sequences'\n",
    "outgroup_protein_fasta_output_loc = output_dir + '/Protein_Sequences_With_Outgroup'\n",
    "sonic_paranoid_run_name = 'Run_Without_Outgroup'\n",
    "outgroup_sonic_paranoid_run_name = 'Run_With_Outgroup'\n",
    "sonic_paranoid_output_loc = output_dir + '/Sonic_Paranoid_Output'\n",
    "ortholog_dir = sonic_paranoid_output_loc + '/runs/' + sonic_paranoid_run_name + '/ortholog_groups'\n",
    "outgroup_ortholog_dir = sonic_paranoid_output_loc + '/runs/' + outgroup_sonic_paranoid_run_name + '/ortholog_groups'\n",
    "non_cds_output_dir = output_dir + '/Multiple_Alignment_Data/Downstream_Non_CDS'\n",
    "upstream_non_cds_output_dir = output_dir + '/Multiple_Alignment_Data/Upstream_Non_CDS'\n",
    "cds_output_dir = output_dir + '/Multiple_Alignment_Data/CDS'\n",
    "extended_cds_output_dir = output_dir + '/Multiple_Alignment_Data/Extended_CDS'\n",
    "outgroup_cds_output_dir = output_dir + '/Multiple_Alignment_Data/CDS_With_Outgroup'\n",
    "outgroup_concatenated_cds_output_dir = output_dir + '/Multiple_Alignment_Data/CDS_With_Outgroup_Concatenated'\n",
    "hmm_parameters_output_dir = output_dir +'/HMM_Model_Parameters'\n",
    "conservation_analysis_output_dir = output_dir + '/Conservation_Analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56139988-efa1-4bb1-9cd3-4cdd90020453",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = 16\n",
    "core_numbers = list(range(1, num_cores+1))\n",
    "non_cds_offset = 50\n",
    "extended_cds_offset = 100\n",
    "tb_species = 'GCF_000195955.2'\n",
    "outgroup_species = 'GCF_000696675.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf829bb3-e529-4170-ad7b-9d885d57f851",
   "metadata": {},
   "source": [
    "##### Determine genomes in ortholog family, generate protein files and run Sonic Paranoid (both with and without outgroup - outgroup needed for tree building)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df7a7627-bb45-4679-97ef-ed8f209447fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_ids_with_outgroup = util.list_dirs(genome_datasets_dir)\n",
    "genome_ids = util.list_dirs(genome_datasets_dir)\n",
    "genome_ids.remove(outgroup_species)\n",
    "non_target_genome_ids = util.list_dirs(genome_datasets_dir)\n",
    "non_target_genome_ids.remove(outgroup_species)\n",
    "non_target_genome_ids.remove(tb_species)\n",
    "num_ids = len(genome_ids)\n",
    "num_ids_with_outgroup = len(genome_ids_with_outgroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be93725-4667-49ca-b2bc-430c0c7e9eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    for folder in sar.tqdm(genome_ids):\n",
    "        sar.generate_protein_file(genome_datasets_dir + '/' + folder + '/genomic.gbff', protein_fasta_output_loc + '/' + folder + '.faa')\n",
    "    for folder in sar.tqdm(genome_ids_with_outgroup):\n",
    "        sar.generate_protein_file(genome_datasets_dir + '/' + folder + '/genomic.gbff', outgroup_protein_fasta_output_loc + '/' + folder + '.faa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828d159-34d6-4f7e-878c-e31b598573a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    sar.run_sonic_paranoid(protein_fasta_output_loc, sonic_paranoid_output_loc, sonic_paranoid_run_name)\n",
    "    sar.run_sonic_paranoid(outgroup_protein_fasta_output_loc, sonic_paranoid_output_loc, outgroup_sonic_paranoid_run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39369771-8cd9-4438-89a5-d426702ac448",
   "metadata": {},
   "source": [
    "##### Generate objects containing orthologs and sequence information for each ortholog group / species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "051a50d7-e6dc-42a7-9675-e58d8ae157b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75075/75075 [00:03<00:00, 23078.71it/s]\n",
      "100%|██████████| 84312/84312 [00:03<00:00, 22307.84it/s]\n"
     ]
    }
   ],
   "source": [
    "orthologs = sar.Ortholog_Grouping(ortholog_dir)\n",
    "outgroup_orthologs = sar.Ortholog_Grouping(outgroup_ortholog_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e9977fb-f334-4f33-a820-535d87f4105a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 250.01it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 7994.86it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 16891.23it/s]\n"
     ]
    }
   ],
   "source": [
    "seq_data = sar.Ortholog_Sequence_Dataset(orthologs, genome_datasets_dir, genome_ids, non_cds_offset, tb_species) \n",
    "outgroup_seq_data = sar.Ortholog_Sequence_Dataset(outgroup_orthologs, genome_datasets_dir, genome_ids_with_outgroup, non_cds_offset, tb_species) \n",
    "all_copy_seq_data = sar.Ortholog_Sequence_Dataset(orthologs, genome_datasets_dir, genome_ids, non_cds_offset, tb_species, single_copy = False) \n",
    "#print(outgroup_seq_data.species_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d5c8c-fdd4-459f-ad82-b74bb46e0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq_data.generate_synteny_plot()\n",
    "#seq_data.generate_ortholog_count_plot()\n",
    "#all_copy_seq_data.generate_master_count_plot()\n",
    "seq_data.generate_unassigned_gene_count_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e3b630-5c42-499e-8389-5532f4dca58c",
   "metadata": {},
   "source": [
    "##### Perform CDS and non-CDS alignments for each full ortholog group and save to folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb33f80-a608-425f-bf01-6a0fe7a77226",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_species = num_ids\n",
    "min_species_with_outgroup = num_ids_with_outgroup\n",
    "groups = random.sample(orthologs.full_single_copy_ortholog_groups, len(orthologs.full_single_copy_ortholog_groups))  #Permutation ensures even distribution of processing speeds\n",
    "outgroup_groups = random.sample(outgroup_orthologs.full_single_copy_ortholog_groups, len(outgroup_orthologs.full_single_copy_ortholog_groups))  #Permutation ensures even distribution of processing speeds\n",
    "if full_run == True:\n",
    "    par = Parallel(n_jobs=-1)(delayed(align.align_and_build)(outgroup_groups, num_cores, core_number, outgroup_seq_data.sequence_data, 'cds_length', 'cds_seq', outgroup_cds_output_dir+'/', min_species_with_outgroup) for core_number in tqdm(core_numbers))\n",
    "    par = Parallel(n_jobs=-1)(delayed(align.align_and_build)(groups, num_cores, core_number, seq_data.sequence_data, 'cds_length', 'cds_seq', cds_output_dir+'/', min_species) for core_number in tqdm(core_numbers))\n",
    "    par = Parallel(n_jobs=-1)(delayed(align.align_and_build)(groups, num_cores, core_number, seq_data.sequence_data, 'non_cds_offset_length', 'non_cds_offset_seq', non_cds_output_dir+'/', min_species) for core_number in tqdm(core_numbers))\n",
    "    par = Parallel(n_jobs=-1)(delayed(align.align_and_build)(groups, num_cores, core_number, seq_data.sequence_data, 'upstream_non_cds_offset_length', 'upstream_non_cds_offset_seq', upstream_non_cds_output_dir+'/', min_species) for core_number in tqdm(core_numbers))\n",
    "    par = Parallel(n_jobs=-1)(delayed(align.align_and_build)(groups, num_cores, core_number, seq_data.sequence_data, 'cds_extended_region_length', 'cds_extended_region_seq', extended_cds_output_dir+'/', min_species) for core_number in tqdm(core_numbers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4552a5b9-a396-450f-956b-0494d74e13af",
   "metadata": {},
   "source": [
    "##### Run IQTree on concatenated CDS alignments to generate tree and rename with full names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42928a48-444f-4b46-a8eb-d668dd4565e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    alignment_names = util.list_files(outgroup_cds_output_dir)\n",
    "    util.concatenate_fasta(outgroup_cds_output_dir, alignment_names, outgroup_concatenated_cds_output_dir + '/concatenated_cds.fasta')\n",
    "    subprocess.run('cd \\\\users\\\\nicho\\\\IQTree & bin\\\\iqtree2 -s ' + outgroup_concatenated_cds_output_dir + '/concatenated_cds.fasta' + ' --prefix '+ output_dir + \n",
    "                   '/Trees/Concatenated_JC_Tree -m JC -B 1000 -T AUTO -o ' + outgroup_species, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1da0b-5a83-47c4-aa6a-77ae862dd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    master_tree = ete3.Tree(output_dir + '/Trees/Concatenated_JC_Tree.treefile')\n",
    "    for node in master_tree.traverse():\n",
    "        if node.is_leaf():\n",
    "            node.name = outgroup_seq_data.organism_dict[node.name] \n",
    "    master_tree.write(format=0, outfile= output_dir + '/Trees/Concatenated_JC_Tree_Full_Names.treefile')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40396ae4-b80d-4ebd-ab66-e40e6712c758",
   "metadata": {},
   "source": [
    "##### Fit overall Alignment and Pairwise HMMs using EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e885b08-1828-4633-9c60-4eeaab22efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_symbols = 4\n",
    "num_states = 3\n",
    "minimum_fit_length = 10\n",
    "initial_params = [0.95, 0.5, 0.95, 0.5, 0.95, 0.5, 0.56370018, 0.52131172, 0.33906948]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7da4636-b1db-4d25-a846-72c8fc4190a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1621/1621 [00:01<00:00, 1500.89it/s]\n"
     ]
    }
   ],
   "source": [
    "Alignment_HMM_Model = alignment_hmm.Alignment_HMM (num_symbols, num_states, non_cds_output_dir, tb_species)\n",
    "if full_run == True:\n",
    "    parameter_fits = Alignment_HMM_Model.EM_update(num_cores, initial_params, non_cds_offset, minimum_fit_length)\n",
    "    fitted_parameters = parameter_fits[3]\n",
    "    with open(hmm_parameters_output_dir + '/' + 'full_parameter_fit.pkl', 'wb') as f:\n",
    "        pickle.dump(fitted_parameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b1b4b3-11cc-4e5a-aea4-3d29f0810066",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    parameter_fits = []\n",
    "    for id in non_target_genome_ids:\n",
    "        parameter_fits.append((id, Alignment_HMM_Model.EM_update(num_cores, initial_params, non_cds_offset, minimum_fit_length, all_species = False, comparison_species = id)[3]))\n",
    "    with open(hmm_parameters_output_dir + '/' + 'pairwise_parameter_fits.pkl', 'wb') as f:\n",
    "        pickle.dump(parameter_fits, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251d95b-1df6-4bc2-8302-52d5c498ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Max likelihood using general optimisation - slower than EM\n",
    "#bound_tuple = [(0.001,0.999),(0.001,0.999),(0.001,0.999),(0.001,0.999),(0.001,0.999),(0.001,0.999),(0.001,0.999),(0.001,0.999),(0.001,0.999)]\n",
    "#def parallel_alignment_hmm_log_likelihood (params):\n",
    "#    core_numbers = range(1, num_cores+1)\n",
    "#    a = Parallel(n_jobs=-1)(delayed(Alignment_HMM_Model.alignment_hmm_log_likelihood)(params, num_cores, core_number, non_cds_offset, minimum_fit_length) for core_number in core_numbers)\n",
    "#    print(params, sum(a))\n",
    "#    return sum(a)  \n",
    "#    res = opt.minimize(parallel_alignment_hmm_log_likelihood, params, method = 'Nelder-Mead', bounds = bound_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75965597-303a-442b-9c61-2f2d1420e1b0",
   "metadata": {},
   "source": [
    "##### Fit overall HMM to pairwise HMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55473b24-e661-4d7f-a9df-e4a485723fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_generate_pairwise_state_probabilities(num_subsets, subset_num, ids, num_states, pairwise_fitted_parameters):\n",
    "    ids = util.chunk_list(ids, num_subsets, subset_num)\n",
    "    pairwise_observation_probabilities = []\n",
    "    for group_id in ids:\n",
    "        temp = []\n",
    "        alignment = align.Alignment(non_cds_output_dir +'/'+str(group_id)+'.fasta', tb_species, 'NT')\n",
    "        alignment.modify_sequence(1,False,False)\n",
    "        for params in pairwise_fitted_parameters:\n",
    "                transition_probabilities, mutation_probabilities = Alignment_HMM_Model.alignment_hmm_model_inputs(params[1])\n",
    "                observation_probabilities = Alignment_HMM_Model.calculate_observation_probs(mutation_probabilities, alignment.modified_sequence_list, alignment, all_species=False, comparison_species = params[0])\n",
    "                initial_state_probabilities = [1.0/num_states]*num_states\n",
    "                hmm_model = hmm.HMM(initial_state_probabilities, transition_probabilities, observation_probabilities)\n",
    "                hmm_model.calculate_probabilities()\n",
    "                temp.append(hmm_model.state_probabilities)\n",
    "        pairwise_observation_probabilities.append(temp) \n",
    "    return pairwise_observation_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce5182ac-6442-40fc-9286-c32fcb173955",
   "metadata": {},
   "outputs": [],
   "source": [
    "if full_run == True:\n",
    "    with open(hmm_parameters_output_dir + '/' + 'pairwise_parameter_fits.pkl', 'rb') as f:\n",
    "        pairwise_fitted_parameters = pickle.load(f)\n",
    "    file_ids = util.list_files(non_cds_output_dir + '/')\n",
    "    ids = [int(i.split('.')[0]) for i in file_ids] \n",
    "    parallel_output = Parallel(n_jobs=-1)(delayed(parallel_generate_pairwise_state_probabilities)(num_cores, core_number, ids, num_states, pairwise_fitted_parameters) for core_number in core_numbers)\n",
    "    pairwise_observation_probabilities = [item for sublist in parallel_output for item in sublist]\n",
    "    with open(hmm_parameters_output_dir + '/' + 'pairwise_observation_probabilities.pkl', 'wb') as f:\n",
    "        pickle.dump(pairwise_observation_probabilities, f)\n",
    "else:\n",
    "    with open(hmm_parameters_output_dir + '/' + 'pairwise_observation_probabilities.pkl', 'rb') as f:\n",
    "        pairwise_observation_probabilities = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "892c503c-dec6-4473-aa3b-38b41ed2b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Master_Alignment_HMM_Model = master_alignment_hmm.Master_Alignment_HMM (pairwise_observation_probabilities)\n",
    "initial_params = [0.8, 0.5, 0.9, 0.3]\n",
    "if full_run == True:\n",
    "    parameter_fits = Master_Alignment_HMM_Model.EM_update(num_cores, initial_params, non_cds_offset, minimum_fit_length)\n",
    "    fitted_parameters = parameter_fits[3]\n",
    "    print(fitted_parameters)\n",
    "    with open(hmm_parameters_output_dir + '/' + 'master_parameter_fit.pkl', 'wb') as f:\n",
    "        pickle.dump(fitted_parameters, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea8afe-116b-4ee5-9b77-f5e3fed532da",
   "metadata": {},
   "source": [
    "##### Load HMM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dea7f2a-cd8b-4d52-bf7b-f3b8bde0536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hmm_parameters_output_dir + '/' + 'full_parameter_fit.pkl', 'rb') as f:\n",
    "    full_fitted_parameters = pickle.load(f)\n",
    "with open(hmm_parameters_output_dir + '/' + 'pairwise_parameter_fits.pkl', 'rb') as f:\n",
    "    pairwise_fitted_parameters = pickle.load(f)\n",
    "with open(hmm_parameters_output_dir + '/' + 'master_parameter_fit.pkl', 'rb') as f:\n",
    "    master_fitted_parameters = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81d481cd-94cd-4192-9359-cc1e598a6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98333407 0.01666593]\n",
      " [0.00569761 0.99430239]] [0.7233083713282976, 0.0595307855532495]\n"
     ]
    }
   ],
   "source": [
    "for params in pairwise_fitted_parameters:\n",
    "    transition_probabilities, mutation_probabilities = copy.deepcopy(Alignment_HMM_Model.alignment_hmm_model_inputs(params[1]))\n",
    "    #print(seq_data.organism_dict[params[0]],transition_probabilities, mutation_probabilities)\n",
    "transition_probabilities, mutation_probabilities = copy.deepcopy(Master_Alignment_HMM_Model.alignment_hmm_model_inputs(master_fitted_parameters))\n",
    "print(transition_probabilities, mutation_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d794a82-bb9b-4718-ba24-a0402f9ab137",
   "metadata": {},
   "source": [
    "##### Analyse ortholog groups for conservation and other features and output to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "507652cd-375b-4fe5-a06b-ae1491be7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_build_analysis_dictionary(num_subsets, subset_num, ids, analysis_type):\n",
    "    ids = util.chunk_list(ids, num_subsets, subset_num)\n",
    "    output_list = []\n",
    "    for group_id in ids:\n",
    "        alignment = align.Alignment(alignment_dir+'/'+str(group_id)+'.fasta', tb_species, 'NT')\n",
    "        analysis = alignment_analysis.Alignment_Analysis(analysis_type, alignment, num_states, non_cds_offset, group_id, full_fitted_parameters, project_dir, Alignment_HMM_Model, \n",
    "                                                         Master_Alignment_HMM_Model, pairwise_fitted_parameters, master_fitted_parameters, seq_data)\n",
    "        output_list.append((group_id, analysis))\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c8f2946-1234-4fb8-acdd-e936b8995dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 5345.19it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 7970.17it/s]\n"
     ]
    }
   ],
   "source": [
    "if full_run == True:\n",
    "    for analysis_type in ['Downstream', 'Upstream']:\n",
    "        if analysis_type == 'Downstream':\n",
    "            alignment_dir = non_cds_output_dir\n",
    "            dict_name = 'downstream_conservation_info_dictionary'\n",
    "        else:\n",
    "            alignment_dir = upstream_non_cds_output_dir\n",
    "            dict_name = 'upstream_conservation_info_dictionary'\n",
    "        alignment_info_dict = {}\n",
    "        file_ids = util.list_files(alignment_dir+'/')\n",
    "        ids = [int(i.split('.')[0]) for i in file_ids]\n",
    "        parallel_output = Parallel(n_jobs=-1)(delayed(parallel_build_analysis_dictionary)(num_cores, core_number, ids, analysis_type) for core_number in tqdm(core_numbers))\n",
    "        dictionary_list = [item for sublist in parallel_output for item in sublist]\n",
    "        alignment_info_dict = {}\n",
    "        for (group, analysis) in dictionary_list:\n",
    "            alignment_info_dict[seq_data.master_species_info(group, 'locus_tag')] = analysis\n",
    "        with open(conservation_analysis_output_dir + '/' + dict_name + '.pkl', 'wb') as f:\n",
    "            pickle.dump(alignment_info_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f9917-0c1c-4e0e-be8c-8f45b7f2c745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e6167-b209-4cef-8583-a1f1edb48769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
